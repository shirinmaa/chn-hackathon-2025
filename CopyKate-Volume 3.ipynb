{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"4030ef77-a92b-4f43-b785-950a0a48791e","cell_type":"code","source":"# Cell 1: We will need the RBCPath type from the rbclib package to load data from the RBC.\nfrom rbclib import RBCPath\n\n# We'll also want to load some data directly from the filesystem.\nfrom pathlib import Path\n\n# We'll want to load/process some of the data using pandas and numpy.\nimport pandas as pd\nimport numpy as np","metadata":{"trusted":true},"outputs":[],"execution_count":6},{"id":"ce020433-8d1c-4ffc-99b0-821097e85acd","cell_type":"code","source":"# Cell 2 (updated)\n# Participant meta-data is generally located in the BIDS repository for each study:\nrbcdata_path = Path('/home/jovyan/shared/data/RBC')\ntrain_filepath = rbcdata_path / 'train_participants.tsv'\ntest_filepath  = rbcdata_path / 'test_participants.tsv'\n\n# Load the PNC participants TSV files...\nwith train_filepath.open('r') as f:\n    train_data = pd.read_csv(f, sep='\\t')\nwith test_filepath.open('r') as f:\n    test_data = pd.read_csv(f, sep='\\t')\n\n# Combine into a single dataframe of all study participants:\nall_data = pd.concat([train_data, test_data], ignore_index=True)\n\n# 1) your known bad participants (missing multiple DKT ROIs)\nparticipants_to_drop = {\n    '1342487188', '1649551035', '2003542642', '219325366', '2249226316',\n    '4184549693', '495793681', '4205323727', '533698126'\n}\n\n# 2) QC-determined bad participants (we will fill this later once we load the QC TSV)\nqc_fail_participants = set()   # <-- placeholder, will update later\n\n# Show the participant table (like the authors did)\nall_data\n","metadata":{"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"      participant_id study study_site session_id  wave        age     sex  \\\n0         1000393599   PNC       PNC1       PNC1     1  15.583333    Male   \n1         1001970838   PNC       PNC1       PNC1     1  17.833333    Male   \n2         1007995238   PNC       PNC1       PNC1     1  13.750000  Female   \n3         1011497669   PNC       PNC1       PNC1     1  16.666667    Male   \n4         1017092387   PNC       PNC1       PNC1     1  18.666667  Female   \n...              ...   ...        ...        ...   ...        ...     ...   \n1596       969649154   PNC       PNC1       PNC1     1  12.333333    Male   \n1597       970890500   PNC       PNC1       PNC1     1  18.166667  Female   \n1598       975856179   PNC       PNC1       PNC1     1  11.000000    Male   \n1599       984757368   PNC       PNC1       PNC1     1  13.416667    Male   \n1600       987544292   PNC       PNC1       PNC1     1  18.416667  Female   \n\n       race               ethnicity    bmi handedness participant_education  \\\n0     Black  not Hispanic or Latino  22.15      Right             9th Grade   \n1     Other      Hispanic or Latino  23.98      Right            11th Grade   \n2     Other  not Hispanic or Latino  23.77      Right             6th Grade   \n3     White  not Hispanic or Latino  29.68      Right             9th Grade   \n4     Black  not Hispanic or Latino  23.24      Right            11th Grade   \n...     ...                     ...    ...        ...                   ...   \n1596  White  not Hispanic or Latino  17.38      Right             5th Grade   \n1597  White  not Hispanic or Latino  30.89      Right            11th Grade   \n1598  White  not Hispanic or Latino  15.67      Right             4th Grade   \n1599  Black  not Hispanic or Latino  16.66      Right             5th Grade   \n1600  White  not Hispanic or Latino    NaN      Right            11th Grade   \n\n      parent_1_education  parent_2_education  p_factor  \n0       Complete primary  Complete secondary  0.589907  \n1      Complete tertiary   Complete tertiary -0.659061  \n2      Complete tertiary    Complete primary -1.608375  \n3      Complete tertiary   Complete tertiary -1.233807  \n4       Complete primary    Complete primary -0.923100  \n...                  ...                 ...       ...  \n1596   Complete tertiary  Complete secondary       NaN  \n1597  Complete secondary  Complete secondary       NaN  \n1598    Complete primary  Complete secondary       NaN  \n1599    Complete primary                 NaN       NaN  \n1600  Complete secondary    Complete primary       NaN  \n\n[1601 rows x 15 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>participant_id</th>\n      <th>study</th>\n      <th>study_site</th>\n      <th>session_id</th>\n      <th>wave</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>race</th>\n      <th>ethnicity</th>\n      <th>bmi</th>\n      <th>handedness</th>\n      <th>participant_education</th>\n      <th>parent_1_education</th>\n      <th>parent_2_education</th>\n      <th>p_factor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000393599</td>\n      <td>PNC</td>\n      <td>PNC1</td>\n      <td>PNC1</td>\n      <td>1</td>\n      <td>15.583333</td>\n      <td>Male</td>\n      <td>Black</td>\n      <td>not Hispanic or Latino</td>\n      <td>22.15</td>\n      <td>Right</td>\n      <td>9th Grade</td>\n      <td>Complete primary</td>\n      <td>Complete secondary</td>\n      <td>0.589907</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1001970838</td>\n      <td>PNC</td>\n      <td>PNC1</td>\n      <td>PNC1</td>\n      <td>1</td>\n      <td>17.833333</td>\n      <td>Male</td>\n      <td>Other</td>\n      <td>Hispanic or Latino</td>\n      <td>23.98</td>\n      <td>Right</td>\n      <td>11th Grade</td>\n      <td>Complete tertiary</td>\n      <td>Complete tertiary</td>\n      <td>-0.659061</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1007995238</td>\n      <td>PNC</td>\n      <td>PNC1</td>\n      <td>PNC1</td>\n      <td>1</td>\n      <td>13.750000</td>\n      <td>Female</td>\n      <td>Other</td>\n      <td>not Hispanic or Latino</td>\n      <td>23.77</td>\n      <td>Right</td>\n      <td>6th Grade</td>\n      <td>Complete tertiary</td>\n      <td>Complete primary</td>\n      <td>-1.608375</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1011497669</td>\n      <td>PNC</td>\n      <td>PNC1</td>\n      <td>PNC1</td>\n      <td>1</td>\n      <td>16.666667</td>\n      <td>Male</td>\n      <td>White</td>\n      <td>not Hispanic or Latino</td>\n      <td>29.68</td>\n      <td>Right</td>\n      <td>9th Grade</td>\n      <td>Complete tertiary</td>\n      <td>Complete tertiary</td>\n      <td>-1.233807</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1017092387</td>\n      <td>PNC</td>\n      <td>PNC1</td>\n      <td>PNC1</td>\n      <td>1</td>\n      <td>18.666667</td>\n      <td>Female</td>\n      <td>Black</td>\n      <td>not Hispanic or Latino</td>\n      <td>23.24</td>\n      <td>Right</td>\n      <td>11th Grade</td>\n      <td>Complete primary</td>\n      <td>Complete primary</td>\n      <td>-0.923100</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1596</th>\n      <td>969649154</td>\n      <td>PNC</td>\n      <td>PNC1</td>\n      <td>PNC1</td>\n      <td>1</td>\n      <td>12.333333</td>\n      <td>Male</td>\n      <td>White</td>\n      <td>not Hispanic or Latino</td>\n      <td>17.38</td>\n      <td>Right</td>\n      <td>5th Grade</td>\n      <td>Complete tertiary</td>\n      <td>Complete secondary</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1597</th>\n      <td>970890500</td>\n      <td>PNC</td>\n      <td>PNC1</td>\n      <td>PNC1</td>\n      <td>1</td>\n      <td>18.166667</td>\n      <td>Female</td>\n      <td>White</td>\n      <td>not Hispanic or Latino</td>\n      <td>30.89</td>\n      <td>Right</td>\n      <td>11th Grade</td>\n      <td>Complete secondary</td>\n      <td>Complete secondary</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1598</th>\n      <td>975856179</td>\n      <td>PNC</td>\n      <td>PNC1</td>\n      <td>PNC1</td>\n      <td>1</td>\n      <td>11.000000</td>\n      <td>Male</td>\n      <td>White</td>\n      <td>not Hispanic or Latino</td>\n      <td>15.67</td>\n      <td>Right</td>\n      <td>4th Grade</td>\n      <td>Complete primary</td>\n      <td>Complete secondary</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1599</th>\n      <td>984757368</td>\n      <td>PNC</td>\n      <td>PNC1</td>\n      <td>PNC1</td>\n      <td>1</td>\n      <td>13.416667</td>\n      <td>Male</td>\n      <td>Black</td>\n      <td>not Hispanic or Latino</td>\n      <td>16.66</td>\n      <td>Right</td>\n      <td>5th Grade</td>\n      <td>Complete primary</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1600</th>\n      <td>987544292</td>\n      <td>PNC</td>\n      <td>PNC1</td>\n      <td>PNC1</td>\n      <td>1</td>\n      <td>18.416667</td>\n      <td>Female</td>\n      <td>White</td>\n      <td>not Hispanic or Latino</td>\n      <td>NaN</td>\n      <td>Right</td>\n      <td>11th Grade</td>\n      <td>Complete secondary</td>\n      <td>Complete primary</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>1601 rows × 15 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"id":"77cdf707-97bc-40e7-93b2-a401a6912bac","cell_type":"code","source":"# Cell 3 (same logic, clearer comment)\ndef load_fsdata(participant_id, local_cache_dir=(Path.home() / 'cache')):\n    \"\"\"\n    Loads and returns the FreeSurfer region-surface-stats TSV\n    for a single PNC participant.\n    We will call this for every participant later and then\n    extract the aparc.DKTatlas rows we care about.\n    \"\"\"\n    if local_cache_dir is not None:\n        local_cache_dir = Path(local_cache_dir)\n        local_cache_dir.mkdir(exist_ok=True)\n\n    pnc_freesurfer_path = RBCPath(\n        'rbc://PNC_FreeSurfer/freesurfer',\n        local_cache_dir=local_cache_dir\n    )\n    participant_path = pnc_freesurfer_path / f'sub-{participant_id}'\n    tsv_path = participant_path / f'sub-{participant_id}_regionsurfacestats.tsv'\n\n    with tsv_path.open('r') as f:\n        data = pd.read_csv(f, sep='\\t')\n\n    return data","metadata":{"trusted":true},"outputs":[],"execution_count":8},{"id":"ad4b154c-a26e-489e-a6c0-e3b2ee3cdd12","cell_type":"code","source":"# Cell 4: Summed (+ normalized) DKT GrayVol for a single participant\n\nimport pandas as pd\nimport numpy as np\n\ndef load_dkt_summed_and_normalized_grayvol(participant_id, local_cache_dir=(Path.home() / 'cache')):\n    \"\"\"\n    Returns a pandas Series with:\n      - total_dkt_grayvol\n      - sum_grayvol_<StructName>  (lh + rh; if only one hemi present, uses that)\n      - norm_grayvol_<StructName> (sum_grayvol_<StructName> / total_dkt_grayvol)\n    Uses only atlas == 'aparc.DKTatlas'.\n    \"\"\"\n    df = load_fsdata(participant_id, local_cache_dir=local_cache_dir)\n\n    # Keep only DKT rows and the columns we need\n    df = df[df['atlas'] == 'aparc.DKTatlas'][['StructName', 'hemisphere', 'GrayVol']].copy()\n\n    if df.empty:\n        # No DKT rows for this participant\n        return pd.Series({'total_dkt_grayvol': np.nan}, dtype=float)\n\n    # Sum across hemispheres per structure (lh + rh). If only one hemi exists, sum = that value.\n    # min_count=1 ensures that if both hemispheres were missing, result is NaN (not 0).\n    summed = df.groupby('StructName', as_index=True)['GrayVol'].sum(min_count=1)\n\n    # Total DKT gray volume (sum over structures, ignoring NaNs)\n    total = summed.sum(min_count=1)\n\n    # Build Series with consistent names\n    summed_named = summed.rename(lambda s: f\"sum_grayvol_{s}\")\n    if pd.isna(total) or total <= 0:\n        normalized_named = summed * np.nan\n    else:\n        normalized_named = (summed / total).rename(lambda s: f\"norm_grayvol_{s}\")\n\n    out = pd.concat([pd.Series({'total_dkt_grayvol': float(total)}), summed_named, normalized_named])\n    out.name = None\n    return out","metadata":{"trusted":true},"outputs":[],"execution_count":9},{"id":"b18e5dba-4d57-473a-b83a-f7ee7080429e","cell_type":"code","source":"# Cell 4_AI: Compute AI for DKT GrayVol per participant\n# AI = (rh - lh) / (rh + lh), NaN if a hemi is missing or denominator <= 0\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\ndef load_dkt_ai_grayvol(participant_id, local_cache_dir=(Path.home() / 'cache')):\n    \"\"\"\n    Returns a pandas Series with:\n      - ai_grayvol_<StructName>  ( (rh - lh) / (rh + lh) )\n    Uses only atlas == 'aparc.DKTatlas'.\n    \"\"\"\n    df = load_fsdata(participant_id, local_cache_dir=local_cache_dir)\n    df = df[df['atlas'] == 'aparc.DKTatlas'][['StructName','hemisphere','GrayVol']].copy()\n    if df.empty:\n        return pd.Series(dtype=float)\n\n    piv = df.pivot_table(index='StructName', columns='hemisphere', values='GrayVol', aggfunc='first')\n    lh = piv.get('lh')\n    rh = piv.get('rh')\n    denom = lh + rh\n    ai = (rh - lh) / denom\n    ai.name = None\n    ai = ai.rename(lambda s: f\"ai_grayvol_{s}\")\n    return ai\n","metadata":{"trusted":true},"outputs":[],"execution_count":40},{"id":"9494ef14-d1d5-4603-9721-8a1bee9639f3","cell_type":"code","source":"# Cell 5: Preview for one example participant\nexample_participant_id = 1000393599  # same as authors' example\nexample_feats = load_dkt_summed_and_normalized_grayvol(example_participant_id)\n\n# Show a few entries\nexample_feats.head(20)","metadata":{"trusted":true},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"total_dkt_grayvol                      541205.0\nsum_grayvol_caudalanteriorcingulate      5585.0\nsum_grayvol_caudalmiddlefrontal         13717.0\nsum_grayvol_cuneus                      11162.0\nsum_grayvol_entorhinal                   5090.0\nsum_grayvol_fusiform                    17342.0\nsum_grayvol_inferiorparietal            32167.0\nsum_grayvol_inferiortemporal            25353.0\nsum_grayvol_insula                      12772.0\nsum_grayvol_isthmuscingulate             5131.0\nsum_grayvol_lateraloccipital            27557.0\nsum_grayvol_lateralorbitofrontal        19203.0\nsum_grayvol_lingual                     14022.0\nsum_grayvol_medialorbitofrontal         10319.0\nsum_grayvol_middletemporal              30510.0\nsum_grayvol_paracentral                  9907.0\nsum_grayvol_parahippocampal              4701.0\nsum_grayvol_parsopercularis              9138.0\nsum_grayvol_parsorbitalis                5015.0\nsum_grayvol_parstriangularis             8414.0\ndtype: float64"},"metadata":{}}],"execution_count":41},{"id":"c112ed57-7df6-4605-b277-97281a4e41e1","cell_type":"code","source":"# Cell 6: Build the full feature table (summed + normalized DKT GrayVol)\n# - Keeps participants_to_drop defined in Cell 2 (we will filter later, after QC load)\n# - Adds p_factor from all_data (NaN for test)\n# - No QC filtering yet; we will merge/ filter after we load QC in a later cell.\n\nprint(\"Building DKT summed & normalized GrayVol features for all participants...\")\n\nfrom ipywidgets import IntProgress\nfrom IPython.display import display\n\nrows = []\nprog = IntProgress(min=0, max=len(all_data))\ndisplay(prog)\n\nfor _, row in all_data.iterrows():\n    pid = str(row['participant_id'])\n    pval = row.get('p_factor', np.nan)\n\n    try:\n        s = load_dkt_summed_and_normalized_grayvol(pid)\n    except FileNotFoundError:\n        # Missing FS TSV for this participant\n        s = pd.Series({'total_dkt_grayvol': np.nan}, dtype=float)\n\n    rec = {'participant_id': pid, 'p_factor': pval}\n    rec.update(s.to_dict())\n    rows.append(rec)\n    prog.value += 1\n\n# Assemble the table\nall_vars_vol = pd.DataFrame(rows)\n\n# (Optional preview) How many feature columns we created?\nsum_cols  = [c for c in all_vars_vol.columns if c.startswith('sum_grayvol_')]\nnorm_cols = [c for c in all_vars_vol.columns if c.startswith('norm_grayvol_')]\nprint(f\"Feature columns: {len(sum_cols)} summed, {len(norm_cols)} normalized; + total_dkt_grayvol\")\n\n# Train/test split mirrors the authors' style\ntrain_vars_vol = all_vars_vol[~np.isnan(all_vars_vol['p_factor'])].reset_index(drop=True)\ntest_vars_vol  = all_vars_vol[ np.isnan(all_vars_vol['p_factor'])].reset_index(drop=True)\n\n# Display the finished dataframe (like the authors)\nall_vars_vol.head()","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Building DKT summed & normalized GrayVol features for all participants...\n"},{"output_type":"display_data","data":{"text/plain":"IntProgress(value=0, max=1601)","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31b75b3035ee46d6a4a216a7b808410b"}},"metadata":{}},{"name":"stdout","output_type":"stream","text":"Feature columns: 33 summed, 33 normalized; + total_dkt_grayvol\n"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"  participant_id  p_factor  total_dkt_grayvol  \\\n0     1000393599  0.589907           541205.0   \n1     1001970838 -0.659061           489732.0   \n2     1007995238 -1.608375           526299.0   \n3     1011497669 -1.233807           535375.0   \n4     1017092387 -0.923100           484183.0   \n\n   sum_grayvol_caudalanteriorcingulate  sum_grayvol_caudalmiddlefrontal  \\\n0                               5585.0                          13717.0   \n1                               4389.0                          12242.0   \n2                               5477.0                          14809.0   \n3                               5906.0                          15392.0   \n4                               5890.0                          14017.0   \n\n   sum_grayvol_cuneus  sum_grayvol_entorhinal  sum_grayvol_fusiform  \\\n0             11162.0                  5090.0               17342.0   \n1              8171.0                  2826.0               15497.0   \n2              8150.0                  3677.0               21217.0   \n3              7839.0                  3272.0               17358.0   \n4              8161.0                  3420.0               13860.0   \n\n   sum_grayvol_inferiorparietal  sum_grayvol_inferiortemporal  ...  \\\n0                       32167.0                       25353.0  ...   \n1                       27638.0                       22427.0  ...   \n2                       36134.0                       24934.0  ...   \n3                       35416.0                       25016.0  ...   \n4                       23049.0                       23166.0  ...   \n\n   norm_grayvol_rostralmiddlefrontal  norm_grayvol_superiorfrontal  \\\n0                           0.046450                      0.113667   \n1                           0.048692                      0.116123   \n2                           0.048746                      0.102833   \n3                           0.054864                      0.100963   \n4                           0.048556                      0.117342   \n\n   norm_grayvol_superiorparietal  norm_grayvol_superiortemporal  \\\n0                       0.042239                       0.074451   \n1                       0.042107                       0.074002   \n2                       0.044271                       0.066384   \n3                       0.044948                       0.063684   \n4                       0.049242                       0.062346   \n\n   norm_grayvol_supramarginal  norm_grayvol_transversetemporal  \\\n0                    0.040988                         0.004608   \n1                    0.047534                         0.004153   \n2                    0.045541                         0.003755   \n3                    0.046203                         0.003396   \n4                    0.042796                         0.004302   \n\n   sum_grayvol_frontalpole  norm_grayvol_frontalpole  \\\n0                      NaN                       NaN   \n1                      NaN                       NaN   \n2                      NaN                       NaN   \n3                      NaN                       NaN   \n4                      NaN                       NaN   \n\n   sum_grayvol_temporalpole  norm_grayvol_temporalpole  \n0                       NaN                        NaN  \n1                       NaN                        NaN  \n2                       NaN                        NaN  \n3                       NaN                        NaN  \n4                       NaN                        NaN  \n\n[5 rows x 69 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>participant_id</th>\n      <th>p_factor</th>\n      <th>total_dkt_grayvol</th>\n      <th>sum_grayvol_caudalanteriorcingulate</th>\n      <th>sum_grayvol_caudalmiddlefrontal</th>\n      <th>sum_grayvol_cuneus</th>\n      <th>sum_grayvol_entorhinal</th>\n      <th>sum_grayvol_fusiform</th>\n      <th>sum_grayvol_inferiorparietal</th>\n      <th>sum_grayvol_inferiortemporal</th>\n      <th>...</th>\n      <th>norm_grayvol_rostralmiddlefrontal</th>\n      <th>norm_grayvol_superiorfrontal</th>\n      <th>norm_grayvol_superiorparietal</th>\n      <th>norm_grayvol_superiortemporal</th>\n      <th>norm_grayvol_supramarginal</th>\n      <th>norm_grayvol_transversetemporal</th>\n      <th>sum_grayvol_frontalpole</th>\n      <th>norm_grayvol_frontalpole</th>\n      <th>sum_grayvol_temporalpole</th>\n      <th>norm_grayvol_temporalpole</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000393599</td>\n      <td>0.589907</td>\n      <td>541205.0</td>\n      <td>5585.0</td>\n      <td>13717.0</td>\n      <td>11162.0</td>\n      <td>5090.0</td>\n      <td>17342.0</td>\n      <td>32167.0</td>\n      <td>25353.0</td>\n      <td>...</td>\n      <td>0.046450</td>\n      <td>0.113667</td>\n      <td>0.042239</td>\n      <td>0.074451</td>\n      <td>0.040988</td>\n      <td>0.004608</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1001970838</td>\n      <td>-0.659061</td>\n      <td>489732.0</td>\n      <td>4389.0</td>\n      <td>12242.0</td>\n      <td>8171.0</td>\n      <td>2826.0</td>\n      <td>15497.0</td>\n      <td>27638.0</td>\n      <td>22427.0</td>\n      <td>...</td>\n      <td>0.048692</td>\n      <td>0.116123</td>\n      <td>0.042107</td>\n      <td>0.074002</td>\n      <td>0.047534</td>\n      <td>0.004153</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1007995238</td>\n      <td>-1.608375</td>\n      <td>526299.0</td>\n      <td>5477.0</td>\n      <td>14809.0</td>\n      <td>8150.0</td>\n      <td>3677.0</td>\n      <td>21217.0</td>\n      <td>36134.0</td>\n      <td>24934.0</td>\n      <td>...</td>\n      <td>0.048746</td>\n      <td>0.102833</td>\n      <td>0.044271</td>\n      <td>0.066384</td>\n      <td>0.045541</td>\n      <td>0.003755</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1011497669</td>\n      <td>-1.233807</td>\n      <td>535375.0</td>\n      <td>5906.0</td>\n      <td>15392.0</td>\n      <td>7839.0</td>\n      <td>3272.0</td>\n      <td>17358.0</td>\n      <td>35416.0</td>\n      <td>25016.0</td>\n      <td>...</td>\n      <td>0.054864</td>\n      <td>0.100963</td>\n      <td>0.044948</td>\n      <td>0.063684</td>\n      <td>0.046203</td>\n      <td>0.003396</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1017092387</td>\n      <td>-0.923100</td>\n      <td>484183.0</td>\n      <td>5890.0</td>\n      <td>14017.0</td>\n      <td>8161.0</td>\n      <td>3420.0</td>\n      <td>13860.0</td>\n      <td>23049.0</td>\n      <td>23166.0</td>\n      <td>...</td>\n      <td>0.048556</td>\n      <td>0.117342</td>\n      <td>0.049242</td>\n      <td>0.062346</td>\n      <td>0.042796</td>\n      <td>0.004302</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 69 columns</p>\n</div>"},"metadata":{}}],"execution_count":11},{"id":"3fe917a4-dd9f-4855-addf-48f30f324c78","cell_type":"code","source":"# Cell 6_AI: Build AI features for all participants (like Cell 6 but using AI)\n\nprint(\"Building DKT AI GrayVol features for all participants...\")\n\nfrom ipywidgets import IntProgress\nfrom IPython.display import display\n\nrows_ai = []\nprog = IntProgress(min=0, max=len(all_data))\ndisplay(prog)\n\nfor _, row in all_data.iterrows():\n    pid = str(row['participant_id'])\n    pval = row.get('p_factor', np.nan)\n    try:\n        s = load_dkt_ai_grayvol(pid)\n    except FileNotFoundError:\n        s = pd.Series(dtype=float)\n\n    rec = {'participant_id': pid, 'p_factor': pval}\n    rec.update(s.to_dict())\n    rows_ai.append(rec)\n    prog.value += 1\n\nall_vars_ai = pd.DataFrame(rows_ai)\n\n# Split like authors\ntrain_vars_ai = all_vars_ai[~np.isnan(all_vars_ai['p_factor'])].reset_index(drop=True)\ntest_vars_ai  = all_vars_ai[ np.isnan(all_vars_ai['p_factor'])].reset_index(drop=True)\n\nprint(\"AI feature columns:\", sum(c.startswith('ai_grayvol_') for c in all_vars_ai.columns))\nall_vars_ai.head()\n","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Building DKT AI GrayVol features for all participants...\n"},{"output_type":"display_data","data":{"text/plain":"IntProgress(value=0, max=1601)","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"707d09a248ed4857874b6ed6f6411d15"}},"metadata":{}},{"name":"stdout","output_type":"stream","text":"AI feature columns: 33\n"},{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"  participant_id  p_factor  ai_grayvol_caudalanteriorcingulate  \\\n0     1000393599  0.589907                           -0.250850   \n1     1001970838 -0.659061                           -0.137845   \n2     1007995238 -1.608375                           -0.226219   \n3     1011497669 -1.233807                           -0.082289   \n4     1017092387 -0.923100                           -0.068930   \n\n   ai_grayvol_caudalmiddlefrontal  ai_grayvol_cuneus  ai_grayvol_entorhinal  \\\n0                       -0.025005          -0.030819              -0.066405   \n1                        0.072047          -0.040020              -0.074310   \n2                       -0.018840          -0.040245              -0.132989   \n3                       -0.116424          -0.107794               0.000000   \n4                       -0.011486           0.044480              -0.028655   \n\n   ai_grayvol_fusiform  ai_grayvol_inferiorparietal  \\\n0             0.056626                     0.159076   \n1            -0.027037                     0.099356   \n2             0.006457                     0.083080   \n3            -0.002189                     0.119438   \n4            -0.026407                     0.078832   \n\n   ai_grayvol_inferiortemporal  ai_grayvol_insula  ...  ai_grayvol_precuneus  \\\n0                     0.063622           0.003288  ...              0.029334   \n1                     0.012886           0.011191  ...              0.057381   \n2                     0.012593           0.039840  ...              0.056477   \n3                    -0.002478           0.040342  ...             -0.006547   \n4                     0.008547          -0.009399  ...             -0.003602   \n\n   ai_grayvol_rostralanteriorcingulate  ai_grayvol_rostralmiddlefrontal  \\\n0                            -0.279949                         0.007836   \n1                            -0.015019                        -0.091839   \n2                            -0.116896                         0.039641   \n3                            -0.086276                        -0.029381   \n4                            -0.162029                        -0.026967   \n\n   ai_grayvol_superiorfrontal  ai_grayvol_superiorparietal  \\\n0                    0.045760                     0.051531   \n1                    0.062266                     0.059599   \n2                    0.036234                     0.023863   \n3                    0.050672                     0.024850   \n4                    0.030150                    -0.023740   \n\n   ai_grayvol_superiortemporal  ai_grayvol_supramarginal  \\\n0                    -0.035316                  0.015282   \n1                    -0.063326                 -0.028137   \n2                    -0.042017                  0.010514   \n3                    -0.040563                 -0.041397   \n4                    -0.035148                 -0.048984   \n\n   ai_grayvol_transversetemporal  ai_grayvol_frontalpole  \\\n0                      -0.140337                     NaN   \n1                      -0.186824                     NaN   \n2                      -0.072874                     NaN   \n3                      -0.129813                     NaN   \n4                      -0.059049                     NaN   \n\n   ai_grayvol_temporalpole  \n0                      NaN  \n1                      NaN  \n2                      NaN  \n3                      NaN  \n4                      NaN  \n\n[5 rows x 35 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>participant_id</th>\n      <th>p_factor</th>\n      <th>ai_grayvol_caudalanteriorcingulate</th>\n      <th>ai_grayvol_caudalmiddlefrontal</th>\n      <th>ai_grayvol_cuneus</th>\n      <th>ai_grayvol_entorhinal</th>\n      <th>ai_grayvol_fusiform</th>\n      <th>ai_grayvol_inferiorparietal</th>\n      <th>ai_grayvol_inferiortemporal</th>\n      <th>ai_grayvol_insula</th>\n      <th>...</th>\n      <th>ai_grayvol_precuneus</th>\n      <th>ai_grayvol_rostralanteriorcingulate</th>\n      <th>ai_grayvol_rostralmiddlefrontal</th>\n      <th>ai_grayvol_superiorfrontal</th>\n      <th>ai_grayvol_superiorparietal</th>\n      <th>ai_grayvol_superiortemporal</th>\n      <th>ai_grayvol_supramarginal</th>\n      <th>ai_grayvol_transversetemporal</th>\n      <th>ai_grayvol_frontalpole</th>\n      <th>ai_grayvol_temporalpole</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000393599</td>\n      <td>0.589907</td>\n      <td>-0.250850</td>\n      <td>-0.025005</td>\n      <td>-0.030819</td>\n      <td>-0.066405</td>\n      <td>0.056626</td>\n      <td>0.159076</td>\n      <td>0.063622</td>\n      <td>0.003288</td>\n      <td>...</td>\n      <td>0.029334</td>\n      <td>-0.279949</td>\n      <td>0.007836</td>\n      <td>0.045760</td>\n      <td>0.051531</td>\n      <td>-0.035316</td>\n      <td>0.015282</td>\n      <td>-0.140337</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1001970838</td>\n      <td>-0.659061</td>\n      <td>-0.137845</td>\n      <td>0.072047</td>\n      <td>-0.040020</td>\n      <td>-0.074310</td>\n      <td>-0.027037</td>\n      <td>0.099356</td>\n      <td>0.012886</td>\n      <td>0.011191</td>\n      <td>...</td>\n      <td>0.057381</td>\n      <td>-0.015019</td>\n      <td>-0.091839</td>\n      <td>0.062266</td>\n      <td>0.059599</td>\n      <td>-0.063326</td>\n      <td>-0.028137</td>\n      <td>-0.186824</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1007995238</td>\n      <td>-1.608375</td>\n      <td>-0.226219</td>\n      <td>-0.018840</td>\n      <td>-0.040245</td>\n      <td>-0.132989</td>\n      <td>0.006457</td>\n      <td>0.083080</td>\n      <td>0.012593</td>\n      <td>0.039840</td>\n      <td>...</td>\n      <td>0.056477</td>\n      <td>-0.116896</td>\n      <td>0.039641</td>\n      <td>0.036234</td>\n      <td>0.023863</td>\n      <td>-0.042017</td>\n      <td>0.010514</td>\n      <td>-0.072874</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1011497669</td>\n      <td>-1.233807</td>\n      <td>-0.082289</td>\n      <td>-0.116424</td>\n      <td>-0.107794</td>\n      <td>0.000000</td>\n      <td>-0.002189</td>\n      <td>0.119438</td>\n      <td>-0.002478</td>\n      <td>0.040342</td>\n      <td>...</td>\n      <td>-0.006547</td>\n      <td>-0.086276</td>\n      <td>-0.029381</td>\n      <td>0.050672</td>\n      <td>0.024850</td>\n      <td>-0.040563</td>\n      <td>-0.041397</td>\n      <td>-0.129813</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1017092387</td>\n      <td>-0.923100</td>\n      <td>-0.068930</td>\n      <td>-0.011486</td>\n      <td>0.044480</td>\n      <td>-0.028655</td>\n      <td>-0.026407</td>\n      <td>0.078832</td>\n      <td>0.008547</td>\n      <td>-0.009399</td>\n      <td>...</td>\n      <td>-0.003602</td>\n      <td>-0.162029</td>\n      <td>-0.026967</td>\n      <td>0.030150</td>\n      <td>-0.023740</td>\n      <td>-0.035148</td>\n      <td>-0.048984</td>\n      <td>-0.059049</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 35 columns</p>\n</div>"},"metadata":{}}],"execution_count":42},{"id":"ef1d0b1a-d6fa-4db7-b50a-c72379c2945d","cell_type":"code","source":"# Cell 7 — Load QC (robust), build drop lists, and filter train/test tables\n# - Drops 9 known-bad participants from BOTH train & test\n# - Drops QC == 'Fail' participants from TRAIN ONLY\n\nfrom rbclib import RBCPath\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\n\ndef load_pnc_t1_qc_robust(local_cache_dir=(Path.home() / 'cache')):\n    \"\"\"\n    Load the PNC structural T1 QC table, trying multiple RBC paths and a GitHub-raw fallback.\n    Returns a dataframe with columns including:\n      - participant_id  (string without 'sub-')\n      - qc_determination (manual decision text)\n      - qc_failed (boolean we set here: True if determination indicates fail/exclude)\n    \"\"\"\n    if local_cache_dir is not None:\n        Path(local_cache_dir).mkdir(exist_ok=True, parents=True)\n\n    # Candidates (some environments only mirror subtrees)\n    rbc_candidates = [\n        ('rbc://PNC_FreeSurfer',            'study-PNC_desc-T1_qc.tsv'),\n        ('rbc://PNC_FreeSurfer/freesurfer', 'study-PNC_desc-T1_qc.tsv'),\n        ('rbc://PNC_BIDS/phenotype',        'study-PNC_desc_T1_qc.tsv'),  # alt naming in some docs\n    ]\n\n    qc = None\n    last_err = None\n    for root, name in rbc_candidates:\n        try:\n            tsv_path = RBCPath(root, local_cache_dir=local_cache_dir) / name\n            with tsv_path.open('r') as f:\n                qc = pd.read_csv(f, sep='\\t')\n            break\n        except Exception as e:\n            last_err = e\n            qc = None\n\n    if qc is None:\n        # Fallback to GitHub raw\n        gh_raw = \"https://raw.githubusercontent.com/ReproBrainChart/PNC_FreeSurfer/main/study-PNC_desc-T1_qc.tsv\"\n        qc = pd.read_csv(gh_raw, sep='\\t')\n\n    # Harmonize IDs\n    if 'participant_id' not in qc.columns:\n        if 'subject_id' in qc.columns:\n            qc['participant_id'] = qc['subject_id'].astype(str).str.replace('^sub-', '', regex=True).str.strip()\n        else:\n            # try to find any column containing 'sub-'\n            sub_cols = [c for c in qc.columns if qc[c].astype(str).str.contains('^sub-').any()]\n            if sub_cols:\n                qc['participant_id'] = qc[sub_cols[0]].astype(str).str.replace('^sub-', '', regex=True).str.strip()\n            else:\n                raise ValueError(\"QC table lacks recognizable subject/participant ID columns.\")\n\n    # Determine failures from the manual decision field\n    # Prefer 'qc_determination' if present; otherwise create a neutral column.\n    if 'qc_determination' in qc.columns:\n        decisions = qc['qc_determination'].astype(str).str.strip().str.lower()\n        qc['qc_failed'] = decisions.isin({'fail', 'failed', 'exclude', 'excluded'})\n    else:\n        qc['qc_failed'] = False  # if column missing, default to no failures\n\n    return qc\n\n# --- Load QC and build the failure set for TRAIN ---\nqc = load_pnc_t1_qc_robust()\n\nqc_fail_participants = set(\n    qc.loc[qc['qc_failed'] == True, 'participant_id'].astype(str).unique().tolist()\n)\n\nprint(f\"QC table loaded. Total rows: {len(qc)}\")\nprint(f\"QC fails detected: {len(qc_fail_participants)}\")\n\n# --- Build global & training-only drop sets ---\n# From Cell 2: participants_to_drop is your fixed list of 9 IDs to remove everywhere\ndrop_ids_global   = set(str(x) for x in participants_to_drop)         # drop from BOTH train & test\ndrop_ids_training = drop_ids_global.union(qc_fail_participants)       # extra QC fails ONLY for train\n\n# --- Apply drops to the feature tables produced in Cell 6 ---\ndef _drop_ids(df, ids):\n    if df is None or len(df) == 0:\n        return df\n    out = df.copy()\n    out['participant_id'] = out['participant_id'].astype(str)\n    return out[~out['participant_id'].isin(ids)].reset_index(drop=True)\n\n# all_vars_vol: drop only your 9 global IDs (keep QC fails for reference)\nall_vars_vol_f   = _drop_ids(all_vars_vol, drop_ids_global)\ntrain_vars_vol_f = _drop_ids(train_vars_vol, drop_ids_training)\ntest_vars_vol_f  = _drop_ids(test_vars_vol,  drop_ids_global)\n\n# --- Quick report ---\ndef _nunique(df): return int(df['participant_id'].nunique()) if df is not None and 'participant_id' in df.columns else 0\n\nprint(\"\\nCounts after filtering:\")\nprint(\"  All (post global drops):\", _nunique(all_vars_vol_f))\nprint(\"  Train (post global + QC):\", _nunique(train_vars_vol_f))\nprint(\"  Test  (post global only):\", _nunique(test_vars_vol_f))\n\n# Keep handy lists of feature columns for later modeling\nsum_cols_f  = [c for c in all_vars_vol_f.columns  if c.startswith('sum_grayvol_')]\nnorm_cols_f = [c for c in all_vars_vol_f.columns  if c.startswith('norm_grayvol_')]\nall_feat_cols = sum_cols_f + norm_cols_f + (['total_dkt_grayvol'] if 'total_dkt_grayvol' in all_vars_vol_f.columns else [])\n\nprint(\"\\nFeature columns prepared:\")\nprint(f\"  Summed: {len(sum_cols_f)} | Normalized: {len(norm_cols_f)} | Has total_dkt_grayvol: {'total_dkt_grayvol' in all_vars_vol_f.columns}\")\n\n# These filtered tables will be the inputs for the next steps (demographics merge + RF CV):\n# - all_vars_vol_f   (combined participants; global drops applied)\n# - train_vars_vol_f (training only; global + QC drops)\n# - test_vars_vol_f  (test only; global drops applied)\n","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"QC table loaded. Total rows: 1592\nQC fails detected: 8\n\nCounts after filtering:\n  All (post global drops): 1592\n  Train (post global + QC): 1054\n  Test  (post global only): 532\n\nFeature columns prepared:\n  Summed: 33 | Normalized: 33 | Has total_dkt_grayvol: True\n"}],"execution_count":12},{"id":"8bfb64f9-548c-43f2-b92d-c39e5501d90a","cell_type":"code","source":"# Cell 7_AI: Filter AI tables using your existing drop sets\n\ndef _ensure_set(x):\n    return set(str(v) for v in x) if x is not None else set()\n\ndrop_ids_global   = _ensure_set(participants_to_drop)                   # from Cell 2\ndrop_ids_training = drop_ids_global.union(_ensure_set(globals().get('qc_fail_participants', set())))\n\ndef _drop_ids(df, ids):\n    out = df.copy()\n    out['participant_id'] = out['participant_id'].astype(str)\n    return out[~out['participant_id'].isin(ids)].reset_index(drop=True)\n\nall_vars_ai_f   = _drop_ids(all_vars_ai, drop_ids_global)\ntrain_vars_ai_f = _drop_ids(train_vars_ai, drop_ids_training)\ntest_vars_ai_f  = _drop_ids(test_vars_ai,  drop_ids_global)\n\ndef _nunique(df): return int(df['participant_id'].nunique()) if 'participant_id' in df.columns else 0\nprint(\"Counts after AI filtering:\")\nprint(\"  All:\",   _nunique(all_vars_ai_f))\nprint(\"  Train:\", _nunique(train_vars_ai_f))\nprint(\"  Test:\",  _nunique(test_vars_ai_f))\n","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Counts after AI filtering:\n  All: 1592\n  Train: 1054\n  Test: 532\n"}],"execution_count":43},{"id":"ad3a4cd4-077b-4438-a382-f4ae90b13b1b","cell_type":"code","source":"# Cell 8 — Demographics update:\n#   • single_parent → categorical ('Yes'/'No')\n#   • remove 'wave' from features\n#   • keep prior logic (drop BMI, fill parent_max_education to min level)\n#   • auto-drop low-coverage brain features (e.g., frontalpole/temporalpole)\n\nimport pandas as pd\nimport numpy as np\n\n# ---- Parent education categories and ordinal mapping ----\nEDU_ORDER = [\n    \"No/incomplete primary\",\n    \"Complete primary\",\n    \"Complete secondary\",\n    \"Complete tertiary\",\n]\nEDU_SCORE = {label: i for i, label in enumerate(EDU_ORDER)}  # 0..3\n\ndef _norm_parent_label(x):\n    if pd.isna(x): return np.nan\n    s = str(x).strip()\n    return s if s in EDU_SCORE else np.nan\n\ndef _edu_score(x):\n    return EDU_SCORE.get(x, np.nan)\n\n# ---- Build demographics with derived variables ----\n# NOTE: 'bmi' intentionally omitted; 'wave' removed per request\ndemo_source_cols = [\n    'age','sex','race','ethnicity','handedness',\n    'participant_education','parent_1_education','parent_2_education',\n    'study_site','study'\n]\ndemo = all_data[['participant_id'] + [c for c in demo_source_cols if c in all_data.columns]].copy()\ndemo['participant_id'] = demo['participant_id'].astype(str)\n\n# Parent labels normalized to the four allowed values\np1 = demo['parent_1_education'].apply(_norm_parent_label) if 'parent_1_education' in demo.columns else pd.Series(np.nan, index=demo.index)\np2 = demo['parent_2_education'].apply(_norm_parent_label) if 'parent_2_education' in demo.columns else pd.Series(np.nan, index=demo.index)\n\n# Max parent education (by ordinal score)\np1_score = p1.apply(_edu_score)\np2_score = p2.apply(_edu_score)\nuse_p1 = p1_score.fillna(-np.inf) >= p2_score.fillna(-np.inf)\ndemo['parent_max_education'] = np.where(use_p1, p1, p2)\n\n# single_parent: 1 if either OR both parent entries are missing; else 0 → then cast to categorical\nsingle_parent_num = np.where(p1.isna() | p2.isna(), 1.0, 0.0)\ndemo['single_parent'] = pd.Series(single_parent_num, index=demo.index).map({1.0: 'Yes', 0.0: 'No'}).astype('category')\n\n# Fill missing parent_max_education with MIN level\ndemo['parent_max_education'] = demo['parent_max_education'].fillna(EDU_ORDER[0]).astype('category')\n\n# Keep only intended demographics (no BMI, no wave)\ndemo_cols = [\n    'age','sex','race','ethnicity','handedness',\n    'participant_education','parent_max_education','single_parent',\n    'study_site','study'\n]\ndemo = demo[['participant_id'] + [c for c in demo_cols if c in demo.columns]]\n\n# ---- Merge demographics into the filtered feature tables from Cell 7 ----\ndef _merge_demo(feat_df):\n    out = feat_df.copy()\n    out['participant_id'] = out['participant_id'].astype(str)\n    return out.merge(demo, on='participant_id', how='left')\n\ntrain_model_df = _merge_demo(train_vars_vol_f)\ntest_model_df  = _merge_demo(test_vars_vol_f)\n\n# ---- Auto-drop low-coverage brain features (on TRAIN only) ----\nnorm_cols_all = [c for c in train_model_df.columns if c.startswith('norm_grayvol_')]\ncoverage = 1.0 - train_model_df[norm_cols_all].isna().mean()\nkeep_norm = coverage[coverage >= 0.95].index.tolist()\ndrop_norm = sorted(set(norm_cols_all) - set(keep_norm))\n\nfeature_cols_brain = keep_norm + (['total_dkt_grayvol'] if 'total_dkt_grayvol' in train_model_df.columns else [])\n\nprint(f\"Train rows: {len(train_model_df)} | Test rows: {len(test_model_df)}\")\nprint(f\"Brain features kept: {len(feature_cols_brain)}\")\nif drop_norm:\n    print(\"Dropped low-coverage features:\", [c.replace('norm_grayvol_', '') for c in drop_norm][:20])\n\n# Final feature list to feed the model (used by Cell 9)\nkeep_cols = feature_cols_brain + [c for c in demo_cols if c in train_model_df.columns]\nprint(\"Example kept brain features:\", [c for c in feature_cols_brain if c.startswith('norm_grayvol_')][:5])\n\n# ---- Missingness BEFORE any imputation (among KEPT columns) ----\nmiss_train = train_model_df[keep_cols + ['p_factor']].isna().sum().sort_values(ascending=False)\nmiss_test  = test_model_df[keep_cols].isna().sum().sort_values(ascending=False)\n\nprint(\"\\nMissing values in TRAIN (per kept column):\")\nprint(miss_train[miss_train > 0].head(40))\nprint(\"\\nMissing values in TEST (per kept column):\")\nprint(miss_test[miss_test > 0].head(40))\n\nrows_with_any_missing_train = train_model_df[keep_cols + ['p_factor']].isna().any(axis=1).sum()\nrows_with_any_missing_test  = test_model_df[keep_cols].isna().any(axis=1).sum()\nprint(f\"\\nRows with ANY missing among KEPT columns (TRAIN): {rows_with_any_missing_train} / {len(train_model_df)}\")\nprint(f\"Rows with ANY missing among KEPT columns (TEST):  {rows_with_any_missing_test} / {len(test_model_df)}\")\n","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Train rows: 1054 | Test rows: 532\nBrain features kept: 32\nDropped low-coverage features: ['frontalpole', 'temporalpole']\nExample kept brain features: ['norm_grayvol_caudalanteriorcingulate', 'norm_grayvol_caudalmiddlefrontal', 'norm_grayvol_cuneus', 'norm_grayvol_entorhinal', 'norm_grayvol_fusiform']\n\nMissing values in TRAIN (per kept column):\nSeries([], dtype: int64)\n\nMissing values in TEST (per kept column):\nSeries([], dtype: int64)\n\nRows with ANY missing among KEPT columns (TRAIN): 0 / 1054\nRows with ANY missing among KEPT columns (TEST):  0 / 532\n"}],"execution_count":17},{"id":"597ff3a6-3abd-4b76-b917-49e90d878c78","cell_type":"code","source":"# Cell 8_AI — Demographics + AI features (fixed parent_max_education construction)\n# - Excludes BMI and wave\n# - single_parent as categorical ('Yes'/'No')\n# - parent_max_education = max(parent_1/2 by ordinal; fill to min level if missing)\n# - Auto-drops low-coverage AI features (>=95% coverage kept)\n# - Produces: train_model_df_ai, test_model_df_ai, feature_cols_brain_ai, keep_cols_ai\n\nimport pandas as pd\nimport numpy as np\n\n# --- Parent education mapping (exact labels you provided) ---\nEDU_ORDER = [\n    \"No/incomplete primary\",\n    \"Complete primary\",\n    \"Complete secondary\",\n    \"Complete tertiary\",\n]\nEDU_SCORE = {label: i for i, label in enumerate(EDU_ORDER)}  # 0..3\n\ndef _norm_parent_label(x):\n    \"\"\"Return the label only if it is one of the four known values; else NaN.\"\"\"\n    if pd.isna(x):\n        return np.nan\n    s = str(x).strip()\n    return s if s in EDU_SCORE else np.nan\n\ndef _edu_score(x):\n    \"\"\"Map label -> ordinal score (0..3); NaN if unknown/missing.\"\"\"\n    return EDU_SCORE.get(x, np.nan)\n\n# --- Build demographics (no BMI, no wave) ---\ndemo_source_cols = [\n    'age','sex','race','ethnicity','handedness',\n    'participant_education','parent_1_education','parent_2_education',\n    'study_site','study'\n]\ndemo = all_data[['participant_id'] + [c for c in demo_source_cols if c in all_data.columns]].copy()\ndemo['participant_id'] = demo['participant_id'].astype(str)\n\n# Normalize parent labels to the four allowed values\np1 = demo['parent_1_education'].apply(_norm_parent_label) if 'parent_1_education' in demo.columns else pd.Series(np.nan, index=demo.index)\np2 = demo['parent_2_education'].apply(_norm_parent_label) if 'parent_2_education' in demo.columns else pd.Series(np.nan, index=demo.index)\n\n# Choose the higher-educated parent by ordinal score (ties -> parent_1; one missing -> the other; both missing -> NaN)\np1_score = p1.apply(_edu_score)\np2_score = p2.apply(_edu_score)\nuse_p1 = p1_score.fillna(-np.inf) >= p2_score.fillna(-np.inf)\n\n# FIX: wrap np.where(...) in a Series before fillna/astype\nparent_choice = pd.Series(np.where(use_p1, p1, p2), index=demo.index, name='parent_max_education')\ndemo['parent_max_education'] = parent_choice.fillna(EDU_ORDER[0]).astype('category')\n\n# single_parent: 'Yes' if either/both parent entries are missing; else 'No' (as categorical)\nsingle_parent_num = np.where(p1.isna() | p2.isna(), 1.0, 0.0)\ndemo['single_parent'] = pd.Series(single_parent_num, index=demo.index).map({1.0: 'Yes', 0.0: 'No'}).astype('category')\n\n# Keep only intended demographics\ndemo_cols = [\n    'age','sex','race','ethnicity','handedness',\n    'participant_education','parent_max_education','single_parent',\n    'study_site','study'\n]\ndemo = demo[['participant_id'] + [c for c in demo_cols if c in demo.columns]]\n\n# --- Merge demographics into the AI feature tables from Cell 7_AI ---\ndef _merge_demo(feat_df):\n    out = feat_df.copy()\n    out['participant_id'] = out['participant_id'].astype(str)\n    return out.merge(demo, on='participant_id', how='left')\n\ntrain_model_df_ai = _merge_demo(train_vars_ai_f)\ntest_model_df_ai  = _merge_demo(test_vars_ai_f)\n\n# --- Auto-drop low-coverage AI features (computed on TRAIN only) ---\nai_cols_all = [c for c in train_model_df_ai.columns if c.startswith('ai_grayvol_')]\ncoverage = 1.0 - train_model_df_ai[ai_cols_all].isna().mean()\nkeep_ai = coverage[coverage >= 0.95].index.tolist()\ndrop_ai = sorted(set(ai_cols_all) - set(keep_ai))\nif drop_ai:\n    print(\"Dropped low-coverage AI features:\", [c.replace('ai_grayvol_','') for c in drop_ai][:20])\n\n# Final brain feature list for the AI path\nfeature_cols_brain_ai = keep_ai\n\n# Columns to feed the model (AI features + demographics)\nkeep_cols_ai = feature_cols_brain_ai + [c for c in demo_cols if c in train_model_df_ai.columns]\n\nprint(f\"AI Train rows: {len(train_model_df_ai)} | AI Test rows: {len(test_model_df_ai)}\")\nprint(f\"AI features kept: {len(feature_cols_brain_ai)}\")\nprint(\"Example kept AI features:\", feature_cols_brain_ai[:5])\n\n# --- Missingness BEFORE any imputation (among KEPT columns) ---\nmiss_train_ai = train_model_df_ai[keep_cols_ai + ['p_factor']].isna().sum().sort_values(ascending=False)\nmiss_test_ai  = test_model_df_ai[keep_cols_ai].isna().sum().sort_values(ascending=False)\n\nprint(\"\\nMissing in TRAIN (AI kept cols):\")\nprint(miss_train_ai[miss_train_ai > 0].head(20))\nprint(\"\\nMissing in TEST (AI kept cols):\")\nprint(miss_test_ai[miss_test_ai > 0].head(20))\n\nrows_with_any_missing_train = train_model_df_ai[keep_cols_ai + ['p_factor']].isna().any(axis=1).sum()\nrows_with_any_missing_test  = test_model_df_ai[keep_cols_ai].isna().any(axis=1).sum()\nprint(f\"\\nRows with ANY missing among KEPT (TRAIN): {rows_with_any_missing_train} / {len(train_model_df_ai)}\")\nprint(f\"Rows with ANY missing among KEPT (TEST):  {rows_with_any_missing_test} / {len(test_model_df_ai)}\")\n","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Dropped low-coverage AI features: ['frontalpole', 'temporalpole']\nAI Train rows: 1054 | AI Test rows: 532\nAI features kept: 31\nExample kept AI features: ['ai_grayvol_caudalanteriorcingulate', 'ai_grayvol_caudalmiddlefrontal', 'ai_grayvol_cuneus', 'ai_grayvol_entorhinal', 'ai_grayvol_fusiform']\n\nMissing in TRAIN (AI kept cols):\nSeries([], dtype: int64)\n\nMissing in TEST (AI kept cols):\nSeries([], dtype: int64)\n\nRows with ANY missing among KEPT (TRAIN): 0 / 1054\nRows with ANY missing among KEPT (TEST):  0 / 532\n"}],"execution_count":45},{"id":"3514a71e-362a-43b7-bc8e-68eb0db73fa8","cell_type":"code","source":"# Cell 8.5 — Unified feature switcher (AI ↔︎ Volume)\n# Sets the canonical: train_model_df, test_model_df, keep_cols\n\nimport pandas as pd\nimport numpy as np\n\n# Choose which feature set to use: 'ai' or 'vol'\nfeature_mode = 'vol'   # <-- set to 'ai' or 'vol'\n\n# Require demographics from Cell 8/8_AI\nassert 'demo' in globals() and 'demo_cols' in globals(), \\\n    \"Missing demographics (demo/demo_cols). Run Cell 8 (or 8_AI) first.\"\n\ndef _merge_demo(feat_df):\n    out = feat_df.copy()\n    out['participant_id'] = out['participant_id'].astype(str)\n    return out.merge(demo, on='participant_id', how='left')\n\nif feature_mode == 'vol':\n    # Need volume feature tables from Cells 6 & 7\n    assert 'train_vars_vol_f' in globals() and 'test_vars_vol_f' in globals(), \\\n        \"Missing volume feature tables. Run Cells 6 & 7.\"\n\n    train_model_df_vol = _merge_demo(train_vars_vol_f)\n    test_model_df_vol  = _merge_demo(test_vars_vol_f)\n\n    # Auto-drop low-coverage normalized VOLUME features (train only)\n    norm_cols_all = [c for c in train_model_df_vol.columns if c.startswith('norm_grayvol_')]\n    coverage = 1.0 - train_model_df_vol[norm_cols_all].isna().mean()\n    keep_norm = coverage[coverage >= 0.95].index.tolist()\n    drop_norm = sorted(set(norm_cols_all) - set(keep_norm))\n\n    feature_cols_brain_vol = keep_norm + (['total_dkt_grayvol'] if 'total_dkt_grayvol' in train_model_df_vol.columns else [])\n    keep_cols_vol = feature_cols_brain_vol + [c for c in demo_cols if c in train_model_df_vol.columns]\n\n    # Set canonical\n    train_model_df = train_model_df_vol\n    test_model_df  = test_model_df_vol\n    keep_cols      = keep_cols_vol\n\n    print(\"Switched to VOLUME features.\")\n    print(f\"Train rows: {len(train_model_df)} | Test rows: {len(test_model_df)}\")\n    print(f\"Brain features kept (vol): {len(feature_cols_brain_vol)}\")\n    if drop_norm:\n        print(\"Dropped low-coverage (vol):\", [c.replace('norm_grayvol_', '') for c in drop_norm][:20])\n\nelif feature_mode == 'ai':\n    # Need AI feature tables from Cells 6_AI & 7_AI\n    assert 'train_vars_ai_f' in globals() and 'test_vars_ai_f' in globals(), \\\n        \"Missing AI feature tables. Run Cells 6_AI & 7_AI.\"\n\n    train_model_df_ai = _merge_demo(train_vars_ai_f)\n    test_model_df_ai  = _merge_demo(test_vars_ai_f)\n\n    # Auto-drop low-coverage AI features (train only)\n    ai_cols_all = [c for c in train_model_df_ai.columns if c.startswith('ai_grayvol_')]\n    coverage = 1.0 - train_model_df_ai[ai_cols_all].isna().mean()\n    keep_ai = coverage[coverage >= 0.95].index.tolist()\n    drop_ai = sorted(set(ai_cols_all) - set(keep_ai))\n\n    feature_cols_brain_ai = keep_ai\n    keep_cols_ai = feature_cols_brain_ai + [c for c in demo_cols if c in train_model_df_ai.columns]\n\n    # Set canonical\n    train_model_df = train_model_df_ai\n    test_model_df  = test_model_df_ai\n    keep_cols      = keep_cols_ai\n\n    print(\"Switched to AI features.\")\n    print(f\"Train rows: {len(train_model_df)} | Test rows: {len(test_model_df)}\")\n    print(f\"Brain features kept (ai): {len(feature_cols_brain_ai)}\")\n    if drop_ai:\n        print(\"Dropped low-coverage (ai):\", [c.replace('ai_grayvol_', '') for c in drop_ai][:20])\n\nelse:\n    raise ValueError(\"feature_mode must be 'vol' or 'ai'\")\n\n# Sanity: show a few feature names and confirm mode\nprint(\"First 5 feature columns:\", keep_cols[:5])\nprint(\"Any AI features in keep_cols? \", any(c.startswith('ai_grayvol_') for c in keep_cols))\nprint(\"Any VOLUME features in keep_cols? \", any(c.startswith('norm_grayvol_') for c in keep_cols))\n","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Switched to VOLUME features.\nTrain rows: 1054 | Test rows: 532\nBrain features kept (vol): 32\nDropped low-coverage (vol): ['frontalpole', 'temporalpole']\nFirst 5 feature columns: ['norm_grayvol_caudalanteriorcingulate', 'norm_grayvol_caudalmiddlefrontal', 'norm_grayvol_cuneus', 'norm_grayvol_entorhinal', 'norm_grayvol_fusiform']\nAny AI features in keep_cols?  False\nAny VOLUME features in keep_cols?  True\n"}],"execution_count":53},{"id":"648fa3b1-64e2-4c62-868d-33a4e0e20848","cell_type":"code","source":"# Inspector — which columns are categorical vs numerical?\n\nimport pandas as pd\nimport numpy as np\n\nX_train = train_model_df[keep_cols].copy()\n\n# Infer types the way scikit-learn will use them\ncat_cols = sorted(X_train.select_dtypes(include=['object', 'category']).columns.tolist())\nnum_cols = sorted(X_train.select_dtypes(include=['number', 'bool']).columns.tolist())\n\nprint(f\"# categorical: {len(cat_cols)}\")\nprint(cat_cols)\nprint(\"\\n# numerical:   {0}\".format(len(num_cols)))\nprint(num_cols)\n\n# Quick dtype summary\nprint(\"\\nDtype counts in X_train:\")\nprint(X_train.dtypes.value_counts())\n\n# Peek at levels of each categorical (first 8 uniques)\nprint(\"\\nCategorical columns (unique values up to 8 each):\")\nfor c in cat_cols:\n    vals = X_train[c].dropna().unique()\n    print(f\"  {c}: {len(vals)} levels -> {vals[:8]}\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"# categorical: 9\n['ethnicity', 'handedness', 'parent_max_education', 'participant_education', 'race', 'sex', 'single_parent', 'study', 'study_site']\n\n# numerical:   33\n['age', 'norm_grayvol_caudalanteriorcingulate', 'norm_grayvol_caudalmiddlefrontal', 'norm_grayvol_cuneus', 'norm_grayvol_entorhinal', 'norm_grayvol_fusiform', 'norm_grayvol_inferiorparietal', 'norm_grayvol_inferiortemporal', 'norm_grayvol_insula', 'norm_grayvol_isthmuscingulate', 'norm_grayvol_lateraloccipital', 'norm_grayvol_lateralorbitofrontal', 'norm_grayvol_lingual', 'norm_grayvol_medialorbitofrontal', 'norm_grayvol_middletemporal', 'norm_grayvol_paracentral', 'norm_grayvol_parahippocampal', 'norm_grayvol_parsopercularis', 'norm_grayvol_parsorbitalis', 'norm_grayvol_parstriangularis', 'norm_grayvol_pericalcarine', 'norm_grayvol_postcentral', 'norm_grayvol_posteriorcingulate', 'norm_grayvol_precentral', 'norm_grayvol_precuneus', 'norm_grayvol_rostralanteriorcingulate', 'norm_grayvol_rostralmiddlefrontal', 'norm_grayvol_superiorfrontal', 'norm_grayvol_superiorparietal', 'norm_grayvol_superiortemporal', 'norm_grayvol_supramarginal', 'norm_grayvol_transversetemporal', 'total_dkt_grayvol']\n\nDtype counts in X_train:\nfloat64     33\nobject       7\ncategory     1\ncategory     1\nName: count, dtype: int64\n\nCategorical columns (unique values up to 8 each):\n  ethnicity: 2 levels -> ['not Hispanic or Latino' 'Hispanic or Latino']\n  handedness: 3 levels -> ['Right' 'Left' 'Ambidextrous']\n  parent_max_education: 4 levels -> ['Complete secondary', 'Complete tertiary', 'Complete primary', 'No/incomplete primary']\nCategories (4, object): ['Complete primary', 'Complete secondary', 'Complete tertiary', 'No/incomplete primary']\n  participant_education: 14 levels -> ['9th Grade' '11th Grade' '6th Grade' '3rd Grade' '4th Grade' '2nd Grade'\n '10th Grade' '12th Grade']\n  race: 4 levels -> ['Black' 'Other' 'White' 'Asian']\n  sex: 2 levels -> ['Male' 'Female']\n  single_parent: 2 levels -> ['No', 'Yes']\nCategories (2, object): ['No', 'Yes']\n  study: 1 levels -> ['PNC']\n  study_site: 1 levels -> ['PNC1']\n"}],"execution_count":54},{"id":"9c83b4ee-b890-41b0-b829-c3143610d61e","cell_type":"code","source":"# Cell 9 — Random Forest with imputers (leakage-safe),\n# (Fixed OneHotEncoder arg for scikit-learn >=1.2 via a compatibility shim.)\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\nimport numpy as np\nimport pandas as pd\n\n# Build X/y from Cell 8 outputs\ny_train = pd.to_numeric(train_model_df['p_factor'], errors='coerce')\nX_train = train_model_df[keep_cols].copy()\n\n# Identify column types (as sklearn will)\ncat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\nnum_cols = X_train.select_dtypes(include=['number', 'bool']).columns.tolist()\n\n# --- OneHotEncoder compatibility shim ---\ntry:\n    # scikit-learn >= 1.2\n    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\nexcept TypeError:\n    # scikit-learn <= 1.1\n    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\n# Preprocess INSIDE the pipeline to avoid leakage:\n#  - Categorical: impute most_frequent, then one-hot (ignore unseen categories)\n#  - Numeric:     impute median (covers any missing that may appear in test)\npre = ColumnTransformer([\n    ('cat', Pipeline(steps=[\n        ('impute', SimpleImputer(strategy='most_frequent')),\n        ('onehot', ohe)\n    ]), cat_cols),\n    ('num', SimpleImputer(strategy='median'), num_cols),\n])\n\n# Random Forest (constrained to reduce overfitting)\nrf = RandomForestRegressor(\n    n_estimators=500,\n    max_depth=4,\n    min_samples_leaf=3,\n    bootstrap=True,\n    oob_score=True,\n    n_jobs=-1,\n    random_state=42,\n)\n\npipe = Pipeline([('prep', pre), ('rf', rf)])\n\n# 5-fold CV on TRAIN ONLY (honest generalization estimate)\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\ncv_r2  = cross_val_score(pipe, X_train, y_train, cv=cv, scoring='r2')\ncv_mae = -cross_val_score(pipe, X_train, y_train, cv=cv, scoring='neg_mean_absolute_error')\n\nprint(\"Random Forest (normalized DKT volumes + total + demographics)\")\nprint(f\"CV R^2 (mean ± sd):  {cv_r2.mean():.3f} ± {cv_r2.std():.3f}\")\nprint(f\"CV MAE (mean ± sd): {cv_mae.mean():.3f} ± {cv_mae.std():.3f}\")\n\n# Fit once on full training set for context (Training R^2 and OOB R^2)\npipe.fit(X_train, y_train)\nprint(f\"Training R^2: {pipe.score(X_train, y_train):.3f}\")\nprint(f\"OOB R^2:      {pipe.named_steps['rf'].oob_score_:.3f}\")\n\n# Keep 'pipe' fitted for the next cell (test predictions)\n","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Random Forest (normalized DKT volumes + total + demographics)\nCV R^2 (mean ± sd):  0.065 ± 0.032\nCV MAE (mean ± sd): 0.751 ± 0.031\nTraining R^2: 0.263\nOOB R^2:      0.064\n"}],"execution_count":56},{"id":"26695eab-9028-4d92-b7bf-29b8ecbad37f","cell_type":"code","source":"# Cell 9b — Tiny sweep with progress bar, auto-detect dataset changes, lock best, fit global `pipe`\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom ipywidgets import IntProgress\nfrom IPython.display import display\n\n# === Configure your grid here ===\ndepths = [3, 4, 5]\nleaves = [2, 3, 4]\nforce_resweep = True   # set True to force a new sweep\n\n# Build train matrices (from previous cells)\ny_train = pd.to_numeric(train_model_df['p_factor'], errors='coerce')\nX_train = train_model_df[keep_cols].copy()\n\n# Column typing\ncat_cols = X_train.select_dtypes(include=['object','category']).columns.tolist()\nnum_cols = X_train.select_dtypes(include=['number','bool']).columns.tolist()\n\n# OneHotEncoder compatibility shim\ntry:\n    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\nexcept TypeError:\n    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\ndef make_preprocessor():\n    return ColumnTransformer([\n        ('cat', Pipeline([\n            ('impute', SimpleImputer(strategy='most_frequent')),\n            ('onehot', ohe)\n        ]), cat_cols),\n        ('num', SimpleImputer(strategy='median'), num_cols),\n    ])\n\n# --- Decide whether to sweep (detect grid OR dataset changes) ---\ngrid_stamp = (tuple(depths), tuple(leaves))\n# dataset stamp: feature columns (order matters) + #rows + quick flag of feature type\nis_ai = any(c.startswith('ai_grayvol_') for c in keep_cols)\ndataset_stamp = (tuple(keep_cols), X_train.shape[0], is_ai)\n\nif '_rf_sweep_cache' not in globals():\n    _rf_sweep_cache = {}\n\nneed_sweep = (\n    force_resweep\n    or ('res_df' not in _rf_sweep_cache)\n    or (_rf_sweep_cache.get('grid_stamp') != grid_stamp)\n    or (_rf_sweep_cache.get('dataset_stamp') != dataset_stamp)\n)\n\nif need_sweep:\n    total = len(depths) * len(leaves)\n    prog = IntProgress(min=0, max=total, description=\"Tuning RF…\")\n    display(prog)\n\n    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n    results = []\n    for depth in depths:\n        for leaf in leaves:\n            rf = RandomForestRegressor(\n                n_estimators=1000,\n                max_depth=depth,\n                min_samples_leaf=leaf,\n                bootstrap=True,\n                oob_score=True,\n                n_jobs=-1,\n                random_state=42,\n            )\n            pipe_tmp = Pipeline([('prep', make_preprocessor()), ('rf', rf)])\n\n            # CV R^2\n            cv_r2 = cross_val_score(pipe_tmp, X_train, y_train, cv=cv, scoring='r2')\n\n            # Fit once for OOB\n            pipe_tmp.fit(X_train, y_train)\n\n            results.append({\n                'max_depth': depth,\n                'min_samples_leaf': leaf,\n                'cv_r2_mean': float(cv_r2.mean()),\n                'cv_r2_sd':   float(cv_r2.std()),\n                'oob_r2':     float(pipe_tmp.named_steps['rf'].oob_score_),\n            })\n            prog.value += 1\n\n    prog.bar_style = 'success'\n    res_df = pd.DataFrame(results).sort_values('cv_r2_mean', ascending=False).reset_index(drop=True)\n    _rf_sweep_cache['res_df'] = res_df\n    _rf_sweep_cache['grid_stamp'] = grid_stamp\n    _rf_sweep_cache['dataset_stamp'] = dataset_stamp\n\n    print(\"Tiny sweep results (sorted by CV R^2 mean):\")\n    print(res_df.to_string(index=False, float_format=lambda v: f\"{v:.3f}\"))\nelse:\n    res_df = _rf_sweep_cache['res_df']\n    print(\"Using cached sweep for grid & dataset.\")\n\n# --- Lock in best hyperparameters from res_df and fit global `pipe` ---\nbest = res_df.iloc[0]\nbest_depth = int(best['max_depth'])\nbest_leaf  = int(best['min_samples_leaf'])\nprint(f\"\\nRefitting with best config: depth={best_depth}, min_samples_leaf={best_leaf}\")\n\npre = make_preprocessor()\nrf = RandomForestRegressor(\n    n_estimators=1000,\n    max_depth=best_depth,\n    min_samples_leaf=best_leaf,\n    bootstrap=True,\n    oob_score=True,\n    n_jobs=-1,\n    random_state=42,\n)\npipe = Pipeline([('prep', pre), ('rf', rf)])\npipe.fit(X_train, y_train)\nprint(f\"Training R^2: {pipe.score(X_train, y_train):.3f}\")\nprint(f\"OOB R^2:      {pipe.named_steps['rf'].oob_score_:.3f}\")\n","metadata":{"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"IntProgress(value=0, description='Tuning RF…', max=9)","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91dd874543074ed1874f6cc334a8c6e9"}},"metadata":{}},{"name":"stdout","output_type":"stream","text":"Tiny sweep results (sorted by CV R^2 mean):\n max_depth  min_samples_leaf  cv_r2_mean  cv_r2_sd  oob_r2\n         4                 2       0.065     0.032   0.063\n         4                 3       0.065     0.032   0.063\n         3                 2       0.064     0.030   0.062\n         5                 2       0.064     0.034   0.063\n         3                 4       0.064     0.030   0.063\n         4                 4       0.064     0.033   0.063\n         3                 3       0.064     0.030   0.063\n         5                 3       0.064     0.035   0.063\n         5                 4       0.063     0.036   0.062\n\nRefitting with best config: depth=4, min_samples_leaf=2\nTraining R^2: 0.261\nOOB R^2:      0.063\n"}],"execution_count":61},{"id":"308685d0-bd87-4dcd-bae8-49e12dfe6702","cell_type":"code","source":"# Cell 10 — Predict p_factor for TEST set and save to CSV\n\nimport pandas as pd\nfrom pathlib import Path\n\n# Use the fitted pipeline `pipe` from Cell 9 and the same feature list `keep_cols` from Cell 8\nX_test = test_model_df[keep_cols].copy()\n\n# Predict p_factor for the test participants\ny_test_pred = pipe.predict(X_test)\n\n# Build predictions dataframe (participant_id + predicted p_factor)\ntest_preds = pd.DataFrame({\n    'participant_id': test_model_df['participant_id'].astype(str).values,\n    'p_factor': y_test_pred\n})\n\n# Save to CSV (no overwrite of any provided files)\nout_path = Path.cwd() / \"test_p_factor_predictions.csv\"\ntest_preds.to_csv(out_path, index=False)\n\nprint(\"Wrote predictions to:\", out_path)\nprint(\"Shape:\", test_preds.shape)\ndisplay(test_preds.head())","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Wrote predictions to: /home/jovyan/chn-hackathon-2025/test_p_factor_predictions.csv\nShape: (532, 2)\n"},{"output_type":"display_data","data":{"text/plain":"  participant_id  p_factor\n0     1000881804 -0.386976\n1      100527940 -0.477904\n2     1006151876  0.001385\n3     1012530688 -0.636700\n4     1030193285 -0.493121","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>participant_id</th>\n      <th>p_factor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000881804</td>\n      <td>-0.386976</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100527940</td>\n      <td>-0.477904</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1006151876</td>\n      <td>0.001385</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1012530688</td>\n      <td>-0.636700</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1030193285</td>\n      <td>-0.493121</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":62},{"id":"23dcf493-bd3e-4d1f-abeb-fb3e43e75c7c","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":37},{"id":"2f712629-1be4-4473-9af5-f8c6d3561932","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}