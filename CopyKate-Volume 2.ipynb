{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"4030ef77-a92b-4f43-b785-950a0a48791e","cell_type":"code","source":"# Cell 1: We will need the RBCPath type from the rbclib package to load data from the RBC.\nfrom rbclib import RBCPath\n\n# We'll also want to load some data directly from the filesystem.\nfrom pathlib import Path\n\n# We'll want to load/process some of the data using pandas and numpy.\nimport pandas as pd\nimport numpy as np","metadata":{"trusted":true},"outputs":[],"execution_count":6},{"id":"ce020433-8d1c-4ffc-99b0-821097e85acd","cell_type":"code","source":"# Cell 2 (updated)\n# Participant meta-data is generally located in the BIDS repository for each study:\nrbcdata_path = Path('/home/jovyan/shared/data/RBC')\ntrain_filepath = rbcdata_path / 'train_participants.tsv'\ntest_filepath  = rbcdata_path / 'test_participants.tsv'\n\n# Load the PNC participants TSV files...\nwith train_filepath.open('r') as f:\n    train_data = pd.read_csv(f, sep='\\t')\nwith test_filepath.open('r') as f:\n    test_data = pd.read_csv(f, sep='\\t')\n\n# Combine into a single dataframe of all study participants:\nall_data = pd.concat([train_data, test_data], ignore_index=True)\n\n# 1) your known bad participants (missing multiple DKT ROIs)\nparticipants_to_drop = {\n    '1342487188', '1649551035', '2003542642', '219325366', '2249226316',\n    '4184549693', '495793681', '4205323727', '533698126'\n}\n\n# 2) QC-determined bad participants (we will fill this later once we load the QC TSV)\nqc_fail_participants = set()   # <-- placeholder, will update later\n\n# Show the participant table (like the authors did)\nall_data\n","metadata":{"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"      participant_id study study_site session_id  wave        age     sex  \\\n0         1000393599   PNC       PNC1       PNC1     1  15.583333    Male   \n1         1001970838   PNC       PNC1       PNC1     1  17.833333    Male   \n2         1007995238   PNC       PNC1       PNC1     1  13.750000  Female   \n3         1011497669   PNC       PNC1       PNC1     1  16.666667    Male   \n4         1017092387   PNC       PNC1       PNC1     1  18.666667  Female   \n...              ...   ...        ...        ...   ...        ...     ...   \n1596       969649154   PNC       PNC1       PNC1     1  12.333333    Male   \n1597       970890500   PNC       PNC1       PNC1     1  18.166667  Female   \n1598       975856179   PNC       PNC1       PNC1     1  11.000000    Male   \n1599       984757368   PNC       PNC1       PNC1     1  13.416667    Male   \n1600       987544292   PNC       PNC1       PNC1     1  18.416667  Female   \n\n       race               ethnicity    bmi handedness participant_education  \\\n0     Black  not Hispanic or Latino  22.15      Right             9th Grade   \n1     Other      Hispanic or Latino  23.98      Right            11th Grade   \n2     Other  not Hispanic or Latino  23.77      Right             6th Grade   \n3     White  not Hispanic or Latino  29.68      Right             9th Grade   \n4     Black  not Hispanic or Latino  23.24      Right            11th Grade   \n...     ...                     ...    ...        ...                   ...   \n1596  White  not Hispanic or Latino  17.38      Right             5th Grade   \n1597  White  not Hispanic or Latino  30.89      Right            11th Grade   \n1598  White  not Hispanic or Latino  15.67      Right             4th Grade   \n1599  Black  not Hispanic or Latino  16.66      Right             5th Grade   \n1600  White  not Hispanic or Latino    NaN      Right            11th Grade   \n\n      parent_1_education  parent_2_education  p_factor  \n0       Complete primary  Complete secondary  0.589907  \n1      Complete tertiary   Complete tertiary -0.659061  \n2      Complete tertiary    Complete primary -1.608375  \n3      Complete tertiary   Complete tertiary -1.233807  \n4       Complete primary    Complete primary -0.923100  \n...                  ...                 ...       ...  \n1596   Complete tertiary  Complete secondary       NaN  \n1597  Complete secondary  Complete secondary       NaN  \n1598    Complete primary  Complete secondary       NaN  \n1599    Complete primary                 NaN       NaN  \n1600  Complete secondary    Complete primary       NaN  \n\n[1601 rows x 15 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>participant_id</th>\n      <th>study</th>\n      <th>study_site</th>\n      <th>session_id</th>\n      <th>wave</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>race</th>\n      <th>ethnicity</th>\n      <th>bmi</th>\n      <th>handedness</th>\n      <th>participant_education</th>\n      <th>parent_1_education</th>\n      <th>parent_2_education</th>\n      <th>p_factor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000393599</td>\n      <td>PNC</td>\n      <td>PNC1</td>\n      <td>PNC1</td>\n      <td>1</td>\n      <td>15.583333</td>\n      <td>Male</td>\n      <td>Black</td>\n      <td>not Hispanic or Latino</td>\n      <td>22.15</td>\n      <td>Right</td>\n      <td>9th Grade</td>\n      <td>Complete primary</td>\n      <td>Complete secondary</td>\n      <td>0.589907</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1001970838</td>\n      <td>PNC</td>\n      <td>PNC1</td>\n      <td>PNC1</td>\n      <td>1</td>\n      <td>17.833333</td>\n      <td>Male</td>\n      <td>Other</td>\n      <td>Hispanic or Latino</td>\n      <td>23.98</td>\n      <td>Right</td>\n      <td>11th Grade</td>\n      <td>Complete tertiary</td>\n      <td>Complete tertiary</td>\n      <td>-0.659061</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1007995238</td>\n      <td>PNC</td>\n      <td>PNC1</td>\n      <td>PNC1</td>\n      <td>1</td>\n      <td>13.750000</td>\n      <td>Female</td>\n      <td>Other</td>\n      <td>not Hispanic or Latino</td>\n      <td>23.77</td>\n      <td>Right</td>\n      <td>6th Grade</td>\n      <td>Complete tertiary</td>\n      <td>Complete primary</td>\n      <td>-1.608375</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1011497669</td>\n      <td>PNC</td>\n      <td>PNC1</td>\n      <td>PNC1</td>\n      <td>1</td>\n      <td>16.666667</td>\n      <td>Male</td>\n      <td>White</td>\n      <td>not Hispanic or Latino</td>\n      <td>29.68</td>\n      <td>Right</td>\n      <td>9th Grade</td>\n      <td>Complete tertiary</td>\n      <td>Complete tertiary</td>\n      <td>-1.233807</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1017092387</td>\n      <td>PNC</td>\n      <td>PNC1</td>\n      <td>PNC1</td>\n      <td>1</td>\n      <td>18.666667</td>\n      <td>Female</td>\n      <td>Black</td>\n      <td>not Hispanic or Latino</td>\n      <td>23.24</td>\n      <td>Right</td>\n      <td>11th Grade</td>\n      <td>Complete primary</td>\n      <td>Complete primary</td>\n      <td>-0.923100</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1596</th>\n      <td>969649154</td>\n      <td>PNC</td>\n      <td>PNC1</td>\n      <td>PNC1</td>\n      <td>1</td>\n      <td>12.333333</td>\n      <td>Male</td>\n      <td>White</td>\n      <td>not Hispanic or Latino</td>\n      <td>17.38</td>\n      <td>Right</td>\n      <td>5th Grade</td>\n      <td>Complete tertiary</td>\n      <td>Complete secondary</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1597</th>\n      <td>970890500</td>\n      <td>PNC</td>\n      <td>PNC1</td>\n      <td>PNC1</td>\n      <td>1</td>\n      <td>18.166667</td>\n      <td>Female</td>\n      <td>White</td>\n      <td>not Hispanic or Latino</td>\n      <td>30.89</td>\n      <td>Right</td>\n      <td>11th Grade</td>\n      <td>Complete secondary</td>\n      <td>Complete secondary</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1598</th>\n      <td>975856179</td>\n      <td>PNC</td>\n      <td>PNC1</td>\n      <td>PNC1</td>\n      <td>1</td>\n      <td>11.000000</td>\n      <td>Male</td>\n      <td>White</td>\n      <td>not Hispanic or Latino</td>\n      <td>15.67</td>\n      <td>Right</td>\n      <td>4th Grade</td>\n      <td>Complete primary</td>\n      <td>Complete secondary</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1599</th>\n      <td>984757368</td>\n      <td>PNC</td>\n      <td>PNC1</td>\n      <td>PNC1</td>\n      <td>1</td>\n      <td>13.416667</td>\n      <td>Male</td>\n      <td>Black</td>\n      <td>not Hispanic or Latino</td>\n      <td>16.66</td>\n      <td>Right</td>\n      <td>5th Grade</td>\n      <td>Complete primary</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1600</th>\n      <td>987544292</td>\n      <td>PNC</td>\n      <td>PNC1</td>\n      <td>PNC1</td>\n      <td>1</td>\n      <td>18.416667</td>\n      <td>Female</td>\n      <td>White</td>\n      <td>not Hispanic or Latino</td>\n      <td>NaN</td>\n      <td>Right</td>\n      <td>11th Grade</td>\n      <td>Complete secondary</td>\n      <td>Complete primary</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>1601 rows × 15 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"id":"77cdf707-97bc-40e7-93b2-a401a6912bac","cell_type":"code","source":"# Cell 3 (same logic, clearer comment)\ndef load_fsdata(participant_id, local_cache_dir=(Path.home() / 'cache')):\n    \"\"\"\n    Loads and returns the FreeSurfer region-surface-stats TSV\n    for a single PNC participant.\n    We will call this for every participant later and then\n    extract the aparc.DKTatlas rows we care about.\n    \"\"\"\n    if local_cache_dir is not None:\n        local_cache_dir = Path(local_cache_dir)\n        local_cache_dir.mkdir(exist_ok=True)\n\n    pnc_freesurfer_path = RBCPath(\n        'rbc://PNC_FreeSurfer/freesurfer',\n        local_cache_dir=local_cache_dir\n    )\n    participant_path = pnc_freesurfer_path / f'sub-{participant_id}'\n    tsv_path = participant_path / f'sub-{participant_id}_regionsurfacestats.tsv'\n\n    with tsv_path.open('r') as f:\n        data = pd.read_csv(f, sep='\\t')\n\n    return data","metadata":{"trusted":true},"outputs":[],"execution_count":8},{"id":"ad4b154c-a26e-489e-a6c0-e3b2ee3cdd12","cell_type":"code","source":"# Cell 4: Summed (+ normalized) DKT GrayVol for a single participant\n\nimport pandas as pd\nimport numpy as np\n\ndef load_dkt_summed_and_normalized_grayvol(participant_id, local_cache_dir=(Path.home() / 'cache')):\n    \"\"\"\n    Returns a pandas Series with:\n      - total_dkt_grayvol\n      - sum_grayvol_<StructName>  (lh + rh; if only one hemi present, uses that)\n      - norm_grayvol_<StructName> (sum_grayvol_<StructName> / total_dkt_grayvol)\n    Uses only atlas == 'aparc.DKTatlas'.\n    \"\"\"\n    df = load_fsdata(participant_id, local_cache_dir=local_cache_dir)\n\n    # Keep only DKT rows and the columns we need\n    df = df[df['atlas'] == 'aparc.DKTatlas'][['StructName', 'hemisphere', 'GrayVol']].copy()\n\n    if df.empty:\n        # No DKT rows for this participant\n        return pd.Series({'total_dkt_grayvol': np.nan}, dtype=float)\n\n    # Sum across hemispheres per structure (lh + rh). If only one hemi exists, sum = that value.\n    # min_count=1 ensures that if both hemispheres were missing, result is NaN (not 0).\n    summed = df.groupby('StructName', as_index=True)['GrayVol'].sum(min_count=1)\n\n    # Total DKT gray volume (sum over structures, ignoring NaNs)\n    total = summed.sum(min_count=1)\n\n    # Build Series with consistent names\n    summed_named = summed.rename(lambda s: f\"sum_grayvol_{s}\")\n    if pd.isna(total) or total <= 0:\n        normalized_named = summed * np.nan\n    else:\n        normalized_named = (summed / total).rename(lambda s: f\"norm_grayvol_{s}\")\n\n    out = pd.concat([pd.Series({'total_dkt_grayvol': float(total)}), summed_named, normalized_named])\n    out.name = None\n    return out","metadata":{"trusted":true},"outputs":[],"execution_count":9},{"id":"9494ef14-d1d5-4603-9721-8a1bee9639f3","cell_type":"code","source":"# Cell 5: Preview for one example participant\nexample_participant_id = 1000393599  # same as authors' example\nexample_feats = load_dkt_summed_and_normalized_grayvol(example_participant_id)\n\n# Show a few entries\nexample_feats.head(20)","metadata":{"trusted":true},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"total_dkt_grayvol                      541205.0\nsum_grayvol_caudalanteriorcingulate      5585.0\nsum_grayvol_caudalmiddlefrontal         13717.0\nsum_grayvol_cuneus                      11162.0\nsum_grayvol_entorhinal                   5090.0\nsum_grayvol_fusiform                    17342.0\nsum_grayvol_inferiorparietal            32167.0\nsum_grayvol_inferiortemporal            25353.0\nsum_grayvol_insula                      12772.0\nsum_grayvol_isthmuscingulate             5131.0\nsum_grayvol_lateraloccipital            27557.0\nsum_grayvol_lateralorbitofrontal        19203.0\nsum_grayvol_lingual                     14022.0\nsum_grayvol_medialorbitofrontal         10319.0\nsum_grayvol_middletemporal              30510.0\nsum_grayvol_paracentral                  9907.0\nsum_grayvol_parahippocampal              4701.0\nsum_grayvol_parsopercularis              9138.0\nsum_grayvol_parsorbitalis                5015.0\nsum_grayvol_parstriangularis             8414.0\ndtype: float64"},"metadata":{}}],"execution_count":10},{"id":"c112ed57-7df6-4605-b277-97281a4e41e1","cell_type":"code","source":"# Cell 6: Build the full feature table (summed + normalized DKT GrayVol)\n# - Keeps participants_to_drop defined in Cell 2 (we will filter later, after QC load)\n# - Adds p_factor from all_data (NaN for test)\n# - No QC filtering yet; we will merge/ filter after we load QC in a later cell.\n\nprint(\"Building DKT summed & normalized GrayVol features for all participants...\")\n\nfrom ipywidgets import IntProgress\nfrom IPython.display import display\n\nrows = []\nprog = IntProgress(min=0, max=len(all_data))\ndisplay(prog)\n\nfor _, row in all_data.iterrows():\n    pid = str(row['participant_id'])\n    pval = row.get('p_factor', np.nan)\n\n    try:\n        s = load_dkt_summed_and_normalized_grayvol(pid)\n    except FileNotFoundError:\n        # Missing FS TSV for this participant\n        s = pd.Series({'total_dkt_grayvol': np.nan}, dtype=float)\n\n    rec = {'participant_id': pid, 'p_factor': pval}\n    rec.update(s.to_dict())\n    rows.append(rec)\n    prog.value += 1\n\n# Assemble the table\nall_vars_vol = pd.DataFrame(rows)\n\n# (Optional preview) How many feature columns we created?\nsum_cols  = [c for c in all_vars_vol.columns if c.startswith('sum_grayvol_')]\nnorm_cols = [c for c in all_vars_vol.columns if c.startswith('norm_grayvol_')]\nprint(f\"Feature columns: {len(sum_cols)} summed, {len(norm_cols)} normalized; + total_dkt_grayvol\")\n\n# Train/test split mirrors the authors' style\ntrain_vars_vol = all_vars_vol[~np.isnan(all_vars_vol['p_factor'])].reset_index(drop=True)\ntest_vars_vol  = all_vars_vol[ np.isnan(all_vars_vol['p_factor'])].reset_index(drop=True)\n\n# Display the finished dataframe (like the authors)\nall_vars_vol.head()","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Building DKT summed & normalized GrayVol features for all participants...\n"},{"output_type":"display_data","data":{"text/plain":"IntProgress(value=0, max=1601)","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31b75b3035ee46d6a4a216a7b808410b"}},"metadata":{}},{"name":"stdout","output_type":"stream","text":"Feature columns: 33 summed, 33 normalized; + total_dkt_grayvol\n"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"  participant_id  p_factor  total_dkt_grayvol  \\\n0     1000393599  0.589907           541205.0   \n1     1001970838 -0.659061           489732.0   \n2     1007995238 -1.608375           526299.0   \n3     1011497669 -1.233807           535375.0   \n4     1017092387 -0.923100           484183.0   \n\n   sum_grayvol_caudalanteriorcingulate  sum_grayvol_caudalmiddlefrontal  \\\n0                               5585.0                          13717.0   \n1                               4389.0                          12242.0   \n2                               5477.0                          14809.0   \n3                               5906.0                          15392.0   \n4                               5890.0                          14017.0   \n\n   sum_grayvol_cuneus  sum_grayvol_entorhinal  sum_grayvol_fusiform  \\\n0             11162.0                  5090.0               17342.0   \n1              8171.0                  2826.0               15497.0   \n2              8150.0                  3677.0               21217.0   \n3              7839.0                  3272.0               17358.0   \n4              8161.0                  3420.0               13860.0   \n\n   sum_grayvol_inferiorparietal  sum_grayvol_inferiortemporal  ...  \\\n0                       32167.0                       25353.0  ...   \n1                       27638.0                       22427.0  ...   \n2                       36134.0                       24934.0  ...   \n3                       35416.0                       25016.0  ...   \n4                       23049.0                       23166.0  ...   \n\n   norm_grayvol_rostralmiddlefrontal  norm_grayvol_superiorfrontal  \\\n0                           0.046450                      0.113667   \n1                           0.048692                      0.116123   \n2                           0.048746                      0.102833   \n3                           0.054864                      0.100963   \n4                           0.048556                      0.117342   \n\n   norm_grayvol_superiorparietal  norm_grayvol_superiortemporal  \\\n0                       0.042239                       0.074451   \n1                       0.042107                       0.074002   \n2                       0.044271                       0.066384   \n3                       0.044948                       0.063684   \n4                       0.049242                       0.062346   \n\n   norm_grayvol_supramarginal  norm_grayvol_transversetemporal  \\\n0                    0.040988                         0.004608   \n1                    0.047534                         0.004153   \n2                    0.045541                         0.003755   \n3                    0.046203                         0.003396   \n4                    0.042796                         0.004302   \n\n   sum_grayvol_frontalpole  norm_grayvol_frontalpole  \\\n0                      NaN                       NaN   \n1                      NaN                       NaN   \n2                      NaN                       NaN   \n3                      NaN                       NaN   \n4                      NaN                       NaN   \n\n   sum_grayvol_temporalpole  norm_grayvol_temporalpole  \n0                       NaN                        NaN  \n1                       NaN                        NaN  \n2                       NaN                        NaN  \n3                       NaN                        NaN  \n4                       NaN                        NaN  \n\n[5 rows x 69 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>participant_id</th>\n      <th>p_factor</th>\n      <th>total_dkt_grayvol</th>\n      <th>sum_grayvol_caudalanteriorcingulate</th>\n      <th>sum_grayvol_caudalmiddlefrontal</th>\n      <th>sum_grayvol_cuneus</th>\n      <th>sum_grayvol_entorhinal</th>\n      <th>sum_grayvol_fusiform</th>\n      <th>sum_grayvol_inferiorparietal</th>\n      <th>sum_grayvol_inferiortemporal</th>\n      <th>...</th>\n      <th>norm_grayvol_rostralmiddlefrontal</th>\n      <th>norm_grayvol_superiorfrontal</th>\n      <th>norm_grayvol_superiorparietal</th>\n      <th>norm_grayvol_superiortemporal</th>\n      <th>norm_grayvol_supramarginal</th>\n      <th>norm_grayvol_transversetemporal</th>\n      <th>sum_grayvol_frontalpole</th>\n      <th>norm_grayvol_frontalpole</th>\n      <th>sum_grayvol_temporalpole</th>\n      <th>norm_grayvol_temporalpole</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000393599</td>\n      <td>0.589907</td>\n      <td>541205.0</td>\n      <td>5585.0</td>\n      <td>13717.0</td>\n      <td>11162.0</td>\n      <td>5090.0</td>\n      <td>17342.0</td>\n      <td>32167.0</td>\n      <td>25353.0</td>\n      <td>...</td>\n      <td>0.046450</td>\n      <td>0.113667</td>\n      <td>0.042239</td>\n      <td>0.074451</td>\n      <td>0.040988</td>\n      <td>0.004608</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1001970838</td>\n      <td>-0.659061</td>\n      <td>489732.0</td>\n      <td>4389.0</td>\n      <td>12242.0</td>\n      <td>8171.0</td>\n      <td>2826.0</td>\n      <td>15497.0</td>\n      <td>27638.0</td>\n      <td>22427.0</td>\n      <td>...</td>\n      <td>0.048692</td>\n      <td>0.116123</td>\n      <td>0.042107</td>\n      <td>0.074002</td>\n      <td>0.047534</td>\n      <td>0.004153</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1007995238</td>\n      <td>-1.608375</td>\n      <td>526299.0</td>\n      <td>5477.0</td>\n      <td>14809.0</td>\n      <td>8150.0</td>\n      <td>3677.0</td>\n      <td>21217.0</td>\n      <td>36134.0</td>\n      <td>24934.0</td>\n      <td>...</td>\n      <td>0.048746</td>\n      <td>0.102833</td>\n      <td>0.044271</td>\n      <td>0.066384</td>\n      <td>0.045541</td>\n      <td>0.003755</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1011497669</td>\n      <td>-1.233807</td>\n      <td>535375.0</td>\n      <td>5906.0</td>\n      <td>15392.0</td>\n      <td>7839.0</td>\n      <td>3272.0</td>\n      <td>17358.0</td>\n      <td>35416.0</td>\n      <td>25016.0</td>\n      <td>...</td>\n      <td>0.054864</td>\n      <td>0.100963</td>\n      <td>0.044948</td>\n      <td>0.063684</td>\n      <td>0.046203</td>\n      <td>0.003396</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1017092387</td>\n      <td>-0.923100</td>\n      <td>484183.0</td>\n      <td>5890.0</td>\n      <td>14017.0</td>\n      <td>8161.0</td>\n      <td>3420.0</td>\n      <td>13860.0</td>\n      <td>23049.0</td>\n      <td>23166.0</td>\n      <td>...</td>\n      <td>0.048556</td>\n      <td>0.117342</td>\n      <td>0.049242</td>\n      <td>0.062346</td>\n      <td>0.042796</td>\n      <td>0.004302</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 69 columns</p>\n</div>"},"metadata":{}}],"execution_count":11},{"id":"ef1d0b1a-d6fa-4db7-b50a-c72379c2945d","cell_type":"code","source":"# Cell 7 — Load QC (robust), build drop lists, and filter train/test tables\n# - Drops 9 known-bad participants from BOTH train & test\n# - Drops QC == 'Fail' participants from TRAIN ONLY\n\nfrom rbclib import RBCPath\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\n\ndef load_pnc_t1_qc_robust(local_cache_dir=(Path.home() / 'cache')):\n    \"\"\"\n    Load the PNC structural T1 QC table, trying multiple RBC paths and a GitHub-raw fallback.\n    Returns a dataframe with columns including:\n      - participant_id  (string without 'sub-')\n      - qc_determination (manual decision text)\n      - qc_failed (boolean we set here: True if determination indicates fail/exclude)\n    \"\"\"\n    if local_cache_dir is not None:\n        Path(local_cache_dir).mkdir(exist_ok=True, parents=True)\n\n    # Candidates (some environments only mirror subtrees)\n    rbc_candidates = [\n        ('rbc://PNC_FreeSurfer',            'study-PNC_desc-T1_qc.tsv'),\n        ('rbc://PNC_FreeSurfer/freesurfer', 'study-PNC_desc-T1_qc.tsv'),\n        ('rbc://PNC_BIDS/phenotype',        'study-PNC_desc_T1_qc.tsv'),  # alt naming in some docs\n    ]\n\n    qc = None\n    last_err = None\n    for root, name in rbc_candidates:\n        try:\n            tsv_path = RBCPath(root, local_cache_dir=local_cache_dir) / name\n            with tsv_path.open('r') as f:\n                qc = pd.read_csv(f, sep='\\t')\n            break\n        except Exception as e:\n            last_err = e\n            qc = None\n\n    if qc is None:\n        # Fallback to GitHub raw\n        gh_raw = \"https://raw.githubusercontent.com/ReproBrainChart/PNC_FreeSurfer/main/study-PNC_desc-T1_qc.tsv\"\n        qc = pd.read_csv(gh_raw, sep='\\t')\n\n    # Harmonize IDs\n    if 'participant_id' not in qc.columns:\n        if 'subject_id' in qc.columns:\n            qc['participant_id'] = qc['subject_id'].astype(str).str.replace('^sub-', '', regex=True).str.strip()\n        else:\n            # try to find any column containing 'sub-'\n            sub_cols = [c for c in qc.columns if qc[c].astype(str).str.contains('^sub-').any()]\n            if sub_cols:\n                qc['participant_id'] = qc[sub_cols[0]].astype(str).str.replace('^sub-', '', regex=True).str.strip()\n            else:\n                raise ValueError(\"QC table lacks recognizable subject/participant ID columns.\")\n\n    # Determine failures from the manual decision field\n    # Prefer 'qc_determination' if present; otherwise create a neutral column.\n    if 'qc_determination' in qc.columns:\n        decisions = qc['qc_determination'].astype(str).str.strip().str.lower()\n        qc['qc_failed'] = decisions.isin({'fail', 'failed', 'exclude', 'excluded'})\n    else:\n        qc['qc_failed'] = False  # if column missing, default to no failures\n\n    return qc\n\n# --- Load QC and build the failure set for TRAIN ---\nqc = load_pnc_t1_qc_robust()\n\nqc_fail_participants = set(\n    qc.loc[qc['qc_failed'] == True, 'participant_id'].astype(str).unique().tolist()\n)\n\nprint(f\"QC table loaded. Total rows: {len(qc)}\")\nprint(f\"QC fails detected: {len(qc_fail_participants)}\")\n\n# --- Build global & training-only drop sets ---\n# From Cell 2: participants_to_drop is your fixed list of 9 IDs to remove everywhere\ndrop_ids_global   = set(str(x) for x in participants_to_drop)         # drop from BOTH train & test\ndrop_ids_training = drop_ids_global.union(qc_fail_participants)       # extra QC fails ONLY for train\n\n# --- Apply drops to the feature tables produced in Cell 6 ---\ndef _drop_ids(df, ids):\n    if df is None or len(df) == 0:\n        return df\n    out = df.copy()\n    out['participant_id'] = out['participant_id'].astype(str)\n    return out[~out['participant_id'].isin(ids)].reset_index(drop=True)\n\n# all_vars_vol: drop only your 9 global IDs (keep QC fails for reference)\nall_vars_vol_f   = _drop_ids(all_vars_vol, drop_ids_global)\ntrain_vars_vol_f = _drop_ids(train_vars_vol, drop_ids_training)\ntest_vars_vol_f  = _drop_ids(test_vars_vol,  drop_ids_global)\n\n# --- Quick report ---\ndef _nunique(df): return int(df['participant_id'].nunique()) if df is not None and 'participant_id' in df.columns else 0\n\nprint(\"\\nCounts after filtering:\")\nprint(\"  All (post global drops):\", _nunique(all_vars_vol_f))\nprint(\"  Train (post global + QC):\", _nunique(train_vars_vol_f))\nprint(\"  Test  (post global only):\", _nunique(test_vars_vol_f))\n\n# Keep handy lists of feature columns for later modeling\nsum_cols_f  = [c for c in all_vars_vol_f.columns  if c.startswith('sum_grayvol_')]\nnorm_cols_f = [c for c in all_vars_vol_f.columns  if c.startswith('norm_grayvol_')]\nall_feat_cols = sum_cols_f + norm_cols_f + (['total_dkt_grayvol'] if 'total_dkt_grayvol' in all_vars_vol_f.columns else [])\n\nprint(\"\\nFeature columns prepared:\")\nprint(f\"  Summed: {len(sum_cols_f)} | Normalized: {len(norm_cols_f)} | Has total_dkt_grayvol: {'total_dkt_grayvol' in all_vars_vol_f.columns}\")\n\n# These filtered tables will be the inputs for the next steps (demographics merge + RF CV):\n# - all_vars_vol_f   (combined participants; global drops applied)\n# - train_vars_vol_f (training only; global + QC drops)\n# - test_vars_vol_f  (test only; global drops applied)\n","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"QC table loaded. Total rows: 1592\nQC fails detected: 8\n\nCounts after filtering:\n  All (post global drops): 1592\n  Train (post global + QC): 1054\n  Test  (post global only): 532\n\nFeature columns prepared:\n  Summed: 33 | Normalized: 33 | Has total_dkt_grayvol: True\n"}],"execution_count":12},{"id":"ad3a4cd4-077b-4438-a382-f4ae90b13b1b","cell_type":"code","source":"# Cell 8 — Demographics update:\n#   • single_parent → categorical ('Yes'/'No')\n#   • remove 'wave' from features\n#   • keep prior logic (drop BMI, fill parent_max_education to min level)\n#   • auto-drop low-coverage brain features (e.g., frontalpole/temporalpole)\n\nimport pandas as pd\nimport numpy as np\n\n# ---- Parent education categories and ordinal mapping ----\nEDU_ORDER = [\n    \"No/incomplete primary\",\n    \"Complete primary\",\n    \"Complete secondary\",\n    \"Complete tertiary\",\n]\nEDU_SCORE = {label: i for i, label in enumerate(EDU_ORDER)}  # 0..3\n\ndef _norm_parent_label(x):\n    if pd.isna(x): return np.nan\n    s = str(x).strip()\n    return s if s in EDU_SCORE else np.nan\n\ndef _edu_score(x):\n    return EDU_SCORE.get(x, np.nan)\n\n# ---- Build demographics with derived variables ----\n# NOTE: 'bmi' intentionally omitted; 'wave' removed per request\ndemo_source_cols = [\n    'age','sex','race','ethnicity','handedness',\n    'participant_education','parent_1_education','parent_2_education',\n    'study_site','study'\n]\ndemo = all_data[['participant_id'] + [c for c in demo_source_cols if c in all_data.columns]].copy()\ndemo['participant_id'] = demo['participant_id'].astype(str)\n\n# Parent labels normalized to the four allowed values\np1 = demo['parent_1_education'].apply(_norm_parent_label) if 'parent_1_education' in demo.columns else pd.Series(np.nan, index=demo.index)\np2 = demo['parent_2_education'].apply(_norm_parent_label) if 'parent_2_education' in demo.columns else pd.Series(np.nan, index=demo.index)\n\n# Max parent education (by ordinal score)\np1_score = p1.apply(_edu_score)\np2_score = p2.apply(_edu_score)\nuse_p1 = p1_score.fillna(-np.inf) >= p2_score.fillna(-np.inf)\ndemo['parent_max_education'] = np.where(use_p1, p1, p2)\n\n# single_parent: 1 if either OR both parent entries are missing; else 0 → then cast to categorical\nsingle_parent_num = np.where(p1.isna() | p2.isna(), 1.0, 0.0)\ndemo['single_parent'] = pd.Series(single_parent_num, index=demo.index).map({1.0: 'Yes', 0.0: 'No'}).astype('category')\n\n# Fill missing parent_max_education with MIN level\ndemo['parent_max_education'] = demo['parent_max_education'].fillna(EDU_ORDER[0]).astype('category')\n\n# Keep only intended demographics (no BMI, no wave)\ndemo_cols = [\n    'age','sex','race','ethnicity','handedness',\n    'participant_education','parent_max_education','single_parent',\n    'study_site','study'\n]\ndemo = demo[['participant_id'] + [c for c in demo_cols if c in demo.columns]]\n\n# ---- Merge demographics into the filtered feature tables from Cell 7 ----\ndef _merge_demo(feat_df):\n    out = feat_df.copy()\n    out['participant_id'] = out['participant_id'].astype(str)\n    return out.merge(demo, on='participant_id', how='left')\n\ntrain_model_df = _merge_demo(train_vars_vol_f)\ntest_model_df  = _merge_demo(test_vars_vol_f)\n\n# ---- Auto-drop low-coverage brain features (on TRAIN only) ----\nnorm_cols_all = [c for c in train_model_df.columns if c.startswith('norm_grayvol_')]\ncoverage = 1.0 - train_model_df[norm_cols_all].isna().mean()\nkeep_norm = coverage[coverage >= 0.95].index.tolist()\ndrop_norm = sorted(set(norm_cols_all) - set(keep_norm))\n\nfeature_cols_brain = keep_norm + (['total_dkt_grayvol'] if 'total_dkt_grayvol' in train_model_df.columns else [])\n\nprint(f\"Train rows: {len(train_model_df)} | Test rows: {len(test_model_df)}\")\nprint(f\"Brain features kept: {len(feature_cols_brain)}\")\nif drop_norm:\n    print(\"Dropped low-coverage features:\", [c.replace('norm_grayvol_', '') for c in drop_norm][:20])\n\n# Final feature list to feed the model (used by Cell 9)\nkeep_cols = feature_cols_brain + [c for c in demo_cols if c in train_model_df.columns]\nprint(\"Example kept brain features:\", [c for c in feature_cols_brain if c.startswith('norm_grayvol_')][:5])\n\n# ---- Missingness BEFORE any imputation (among KEPT columns) ----\nmiss_train = train_model_df[keep_cols + ['p_factor']].isna().sum().sort_values(ascending=False)\nmiss_test  = test_model_df[keep_cols].isna().sum().sort_values(ascending=False)\n\nprint(\"\\nMissing values in TRAIN (per kept column):\")\nprint(miss_train[miss_train > 0].head(40))\nprint(\"\\nMissing values in TEST (per kept column):\")\nprint(miss_test[miss_test > 0].head(40))\n\nrows_with_any_missing_train = train_model_df[keep_cols + ['p_factor']].isna().any(axis=1).sum()\nrows_with_any_missing_test  = test_model_df[keep_cols].isna().any(axis=1).sum()\nprint(f\"\\nRows with ANY missing among KEPT columns (TRAIN): {rows_with_any_missing_train} / {len(train_model_df)}\")\nprint(f\"Rows with ANY missing among KEPT columns (TEST):  {rows_with_any_missing_test} / {len(test_model_df)}\")\n","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Train rows: 1054 | Test rows: 532\nBrain features kept: 32\nDropped low-coverage features: ['frontalpole', 'temporalpole']\nExample kept brain features: ['norm_grayvol_caudalanteriorcingulate', 'norm_grayvol_caudalmiddlefrontal', 'norm_grayvol_cuneus', 'norm_grayvol_entorhinal', 'norm_grayvol_fusiform']\n\nMissing values in TRAIN (per kept column):\nSeries([], dtype: int64)\n\nMissing values in TEST (per kept column):\nSeries([], dtype: int64)\n\nRows with ANY missing among KEPT columns (TRAIN): 0 / 1054\nRows with ANY missing among KEPT columns (TEST):  0 / 532\n"}],"execution_count":17},{"id":"648fa3b1-64e2-4c62-868d-33a4e0e20848","cell_type":"code","source":"# Inspector — which columns are categorical vs numerical?\n\nimport pandas as pd\nimport numpy as np\n\nX_train = train_model_df[keep_cols].copy()\n\n# Infer types the way scikit-learn will use them\ncat_cols = sorted(X_train.select_dtypes(include=['object', 'category']).columns.tolist())\nnum_cols = sorted(X_train.select_dtypes(include=['number', 'bool']).columns.tolist())\n\nprint(f\"# categorical: {len(cat_cols)}\")\nprint(cat_cols)\nprint(\"\\n# numerical:   {0}\".format(len(num_cols)))\nprint(num_cols)\n\n# Quick dtype summary\nprint(\"\\nDtype counts in X_train:\")\nprint(X_train.dtypes.value_counts())\n\n# Peek at levels of each categorical (first 8 uniques)\nprint(\"\\nCategorical columns (unique values up to 8 each):\")\nfor c in cat_cols:\n    vals = X_train[c].dropna().unique()\n    print(f\"  {c}: {len(vals)} levels -> {vals[:8]}\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"# categorical: 9\n['ethnicity', 'handedness', 'parent_max_education', 'participant_education', 'race', 'sex', 'single_parent', 'study', 'study_site']\n\n# numerical:   33\n['age', 'norm_grayvol_caudalanteriorcingulate', 'norm_grayvol_caudalmiddlefrontal', 'norm_grayvol_cuneus', 'norm_grayvol_entorhinal', 'norm_grayvol_fusiform', 'norm_grayvol_inferiorparietal', 'norm_grayvol_inferiortemporal', 'norm_grayvol_insula', 'norm_grayvol_isthmuscingulate', 'norm_grayvol_lateraloccipital', 'norm_grayvol_lateralorbitofrontal', 'norm_grayvol_lingual', 'norm_grayvol_medialorbitofrontal', 'norm_grayvol_middletemporal', 'norm_grayvol_paracentral', 'norm_grayvol_parahippocampal', 'norm_grayvol_parsopercularis', 'norm_grayvol_parsorbitalis', 'norm_grayvol_parstriangularis', 'norm_grayvol_pericalcarine', 'norm_grayvol_postcentral', 'norm_grayvol_posteriorcingulate', 'norm_grayvol_precentral', 'norm_grayvol_precuneus', 'norm_grayvol_rostralanteriorcingulate', 'norm_grayvol_rostralmiddlefrontal', 'norm_grayvol_superiorfrontal', 'norm_grayvol_superiorparietal', 'norm_grayvol_superiortemporal', 'norm_grayvol_supramarginal', 'norm_grayvol_transversetemporal', 'total_dkt_grayvol']\n\nDtype counts in X_train:\nfloat64     33\nobject       7\ncategory     1\ncategory     1\nName: count, dtype: int64\n\nCategorical columns (unique values up to 8 each):\n  ethnicity: 2 levels -> ['not Hispanic or Latino' 'Hispanic or Latino']\n  handedness: 3 levels -> ['Right' 'Left' 'Ambidextrous']\n  parent_max_education: 4 levels -> ['Complete secondary', 'Complete tertiary', 'Complete primary', 'No/incomplete primary']\nCategories (4, object): ['Complete primary', 'Complete secondary', 'Complete tertiary', 'No/incomplete primary']\n  participant_education: 14 levels -> ['9th Grade' '11th Grade' '6th Grade' '3rd Grade' '4th Grade' '2nd Grade'\n '10th Grade' '12th Grade']\n  race: 4 levels -> ['Black' 'Other' 'White' 'Asian']\n  sex: 2 levels -> ['Male' 'Female']\n  single_parent: 2 levels -> ['No', 'Yes']\nCategories (2, object): ['No', 'Yes']\n  study: 1 levels -> ['PNC']\n  study_site: 1 levels -> ['PNC1']\n"}],"execution_count":18},{"id":"9c83b4ee-b890-41b0-b829-c3143610d61e","cell_type":"code","source":"# Cell 9 — Random Forest with imputers (leakage-safe), honest evaluation (CV) + OOB\n# (Fixed OneHotEncoder arg for scikit-learn >=1.2 via a compatibility shim.)\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\nimport numpy as np\nimport pandas as pd\n\n# Build X/y from Cell 8 outputs\ny_train = pd.to_numeric(train_model_df['p_factor'], errors='coerce')\nX_train = train_model_df[keep_cols].copy()\n\n# Identify column types (as sklearn will)\ncat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\nnum_cols = X_train.select_dtypes(include=['number', 'bool']).columns.tolist()\n\n# --- OneHotEncoder compatibility shim ---\ntry:\n    # scikit-learn >= 1.2\n    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\nexcept TypeError:\n    # scikit-learn <= 1.1\n    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\n# Preprocess INSIDE the pipeline to avoid leakage:\n#  - Categorical: impute most_frequent, then one-hot (ignore unseen categories)\n#  - Numeric:     impute median (covers any missing that may appear in test)\npre = ColumnTransformer([\n    ('cat', Pipeline(steps=[\n        ('impute', SimpleImputer(strategy='most_frequent')),\n        ('onehot', ohe)\n    ]), cat_cols),\n    ('num', SimpleImputer(strategy='median'), num_cols),\n])\n\n# Random Forest (constrained to reduce overfitting)\nrf = RandomForestRegressor(\n    n_estimators=500,\n    max_depth=6,\n    min_samples_leaf=5,\n    bootstrap=True,\n    oob_score=True,\n    n_jobs=-1,\n    random_state=42,\n)\n\npipe = Pipeline([('prep', pre), ('rf', rf)])\n\n# 5-fold CV on TRAIN ONLY (honest generalization estimate)\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\ncv_r2  = cross_val_score(pipe, X_train, y_train, cv=cv, scoring='r2')\ncv_mae = -cross_val_score(pipe, X_train, y_train, cv=cv, scoring='neg_mean_absolute_error')\n\nprint(\"Random Forest (normalized DKT volumes + total + demographics)\")\nprint(f\"CV R^2 (mean ± sd):  {cv_r2.mean():.3f} ± {cv_r2.std():.3f}\")\nprint(f\"CV MAE (mean ± sd): {cv_mae.mean():.3f} ± {cv_mae.std():.3f}\")\n\n# Fit once on full training set for context (Training R^2 and OOB R^2)\npipe.fit(X_train, y_train)\nprint(f\"Training R^2: {pipe.score(X_train, y_train):.3f}\")\nprint(f\"OOB R^2:      {pipe.named_steps['rf'].oob_score_:.3f}\")\n\n# Keep 'pipe' fitted for the next cell (test predictions)\n","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Random Forest (normalized DKT volumes + total + demographics)\nCV R^2 (mean ± sd):  0.060 ± 0.037\nCV MAE (mean ± sd): 0.751 ± 0.032\nTraining R^2: 0.439\nOOB R^2:      0.062\n"}],"execution_count":27},{"id":"26695eab-9028-4d92-b7bf-29b8ecbad37f","cell_type":"code","source":"# Cell 9b — Tiny sweep with progress bar, force-resweep & grid detection\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom ipywidgets import IntProgress\nfrom IPython.display import display\n\n# === Configure your grid here ===\ndepths = [3, 4, 5]      # <- change these to new values when you want\nleaves = [3, 5]        # <- change these to new values when you want\nforce_resweep = False   # <- set True to force running the sweep\n\n# Build train matrices (from previous cells)\ny_train = pd.to_numeric(train_model_df['p_factor'], errors='coerce')\nX_train = train_model_df[keep_cols].copy()\n\n# Column typing\ncat_cols = X_train.select_dtypes(include=['object','category']).columns.tolist()\nnum_cols = X_train.select_dtypes(include=['number','bool']).columns.tolist()\n\n# OneHotEncoder compatibility shim\ntry:\n    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\nexcept TypeError:\n    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\ndef make_preprocessor():\n    return ColumnTransformer([\n        ('cat', Pipeline([\n            ('impute', SimpleImputer(strategy='most_frequent')),\n            ('onehot', ohe)\n        ]), cat_cols),\n        ('num', SimpleImputer(strategy='median'), num_cols),\n    ])\n\n# Decide whether to sweep\ngrid_stamp = (tuple(depths), tuple(leaves))\nneed_sweep = (\n    force_resweep\n    or ('res_df' not in globals())\n    or ('_rf_grid_stamp' not in globals())\n    or (globals()['_rf_grid_stamp'] != grid_stamp)\n)\n\nif need_sweep:\n    total = len(depths) * len(leaves)\n    prog = IntProgress(min=0, max=total, description=\"Tuning RF…\")\n    display(prog)\n\n    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n    results = []\n    for depth in depths:\n        for leaf in leaves:\n            rf = RandomForestRegressor(\n                n_estimators=500,\n                max_depth=depth,\n                min_samples_leaf=leaf,\n                bootstrap=True,\n                oob_score=True,\n                n_jobs=-1,\n                random_state=42,\n            )\n            pipe_tmp = Pipeline([('prep', make_preprocessor()), ('rf', rf)])\n\n            # CV R^2\n            cv_r2 = cross_val_score(pipe_tmp, X_train, y_train, cv=cv, scoring='r2')\n            # Fit once for OOB\n            pipe_tmp.fit(X_train, y_train)\n\n            results.append({\n                'max_depth': depth,\n                'min_samples_leaf': leaf,\n                'cv_r2_mean': float(cv_r2.mean()),\n                'cv_r2_sd':   float(cv_r2.std()),\n                'oob_r2':     float(pipe_tmp.named_steps['rf'].oob_score_),\n            })\n            prog.value += 1\n\n    prog.bar_style = 'success'\n    res_df = pd.DataFrame(results).sort_values('cv_r2_mean', ascending=False).reset_index(drop=True)\n    _rf_grid_stamp = grid_stamp  # remember which grid produced res_df\n    print(\"Tiny sweep results (sorted by CV R^2 mean):\")\n    print(res_df.to_string(index=False, float_format=lambda v: f\"{v:.3f}\"))\nelse:\n    print(\"Using existing sweep results for grid:\", globals().get('_rf_grid_stamp', None))\n\n# Lock in best hyperparameters from res_df and fit global `pipe`\nbest = res_df.iloc[0]\nbest_depth = int(best['max_depth'])\nbest_leaf  = int(best['min_samples_leaf'])\nprint(f\"\\nRefitting with best config: depth={best_depth}, min_samples_leaf={best_leaf}\")\n\npre = make_preprocessor()\nrf = RandomForestRegressor(\n    n_estimators=500,\n    max_depth=best_depth,\n    min_samples_leaf=best_leaf,\n    bootstrap=True,\n    oob_score=True,\n    n_jobs=-1,\n    random_state=42,\n)\npipe = Pipeline([('prep', pre), ('rf', rf)])\npipe.fit(X_train, y_train)\nprint(f\"Training R^2: {pipe.score(X_train, y_train):.3f}\")\nprint(f\"OOB R^2:      {pipe.named_steps['rf'].oob_score_:.3f}\")\n","metadata":{"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"IntProgress(value=0, description='Tuning RF…', max=6)","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06642fceccff4bbca0c1e52bdbff3291"}},"metadata":{}},{"name":"stdout","output_type":"stream","text":"Tiny sweep results (sorted by CV R^2 mean):\n max_depth  min_samples_leaf  cv_r2_mean  cv_r2_sd  oob_r2\n         4                 3       0.065     0.032   0.064\n         3                 3       0.064     0.030   0.063\n         5                 3       0.064     0.034   0.063\n         3                 5       0.064     0.031   0.063\n         4                 5       0.064     0.034   0.063\n         5                 5       0.062     0.035   0.062\n\nRefitting with best config: depth=4, min_samples_leaf=3\nTraining R^2: 0.263\nOOB R^2:      0.064\n"}],"execution_count":35},{"id":"308685d0-bd87-4dcd-bae8-49e12dfe6702","cell_type":"code","source":"# Cell 10 — Predict p_factor for TEST set and save to CSV\n\nimport pandas as pd\nfrom pathlib import Path\n\n# Use the fitted pipeline `pipe` from Cell 9 and the same feature list `keep_cols` from Cell 8\nX_test = test_model_df[keep_cols].copy()\n\n# Predict p_factor for the test participants\ny_test_pred = pipe.predict(X_test)\n\n# Build predictions dataframe (participant_id + predicted p_factor)\ntest_preds = pd.DataFrame({\n    'participant_id': test_model_df['participant_id'].astype(str).values,\n    'p_factor': y_test_pred\n})\n\n# Save to CSV (no overwrite of any provided files)\nout_path = Path.cwd() / \"test_p_factor_predictions.csv\"\ntest_preds.to_csv(out_path, index=False)\n\nprint(\"Wrote predictions to:\", out_path)\nprint(\"Shape:\", test_preds.shape)\ndisplay(test_preds.head())","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Wrote predictions to: /home/jovyan/chn-hackathon-2025/test_p_factor_predictions.csv\nShape: (532, 2)\n"},{"output_type":"display_data","data":{"text/plain":"  participant_id  p_factor\n0     1000881804 -0.392259\n1      100527940 -0.480867\n2     1006151876  0.010003\n3     1012530688 -0.663160\n4     1030193285 -0.489929","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>participant_id</th>\n      <th>p_factor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000881804</td>\n      <td>-0.392259</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100527940</td>\n      <td>-0.480867</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1006151876</td>\n      <td>0.010003</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1012530688</td>\n      <td>-0.663160</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1030193285</td>\n      <td>-0.489929</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":36},{"id":"23dcf493-bd3e-4d1f-abeb-fb3e43e75c7c","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}